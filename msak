#!/usr/bin/env python3
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

import aiohttp
import asyncio
import json
import argparse
import os
import yaml
import logging
import re
import requests
import time
import pprint
import sys
import traceback
from dictdiffer import diff
import jsonschema
from jsonschema import validate
from aiohttp.client_exceptions import ClientError
import pandas as pd
import meraki

DEFAULT_MERAKI_BASE_URL = 'https://api.meraki.com/api/v1'
CONFIG_FILES = ['/etc/meraki/meraki_inventory.yml', '/etc/ansible/meraki_inventory.yml']
OPENAPI_SPEC_FILE = 'meraki-openapi-spec.json'
API_MAX_RETRIES             = 3
API_CONNECT_TIMEOUT         = 60
API_TRANSMIT_TIMEOUT        = 60
API_STATUS_RATE_LIMIT       = 429
API_RETRY_DEFAULT_WAIT      = 3

PRODUCT_TYPES = [
        'appliance',
        'switch',
        'wireless',
        'sensor',
        'camera',
        'cellularGateway'
]

PRODUCT_TYPES_MAP = {
    'appliance': ['MX'],
    'wireless': ['MR', 'CW'],
    'switch': ['MX']
}

readonly_exceptions = [
    "/organizations/{organizationId}/inventory/devices"
]

index_lookup = {
    "/networks/{networkId}/switch/accessPolicies": "accessPolicyNumber",
    "/networks/{networkId}/wireless/ssids": "number",
    "/networks/{networkId}/groupPolicies": "groupPolicyId",
    "/networks/{networkId}/appliance/vlans": "vlanId",
    "/devices/{serial}/switch/routing/interfaces": "interfaceId",
    "/networks/{networkId}/switch/stacks/{switchStackId}/routing/interfaces": "interfaceId",
    "/networks/{networkId}/vlanProfiles": "iname",
    "/devices/{serial}/switch/routing/staticRoutes": "staticRouteId",
    "/networks/{networkId}/switch/stacks/{switchStackId}/routing/staticRoutes": "staticRouteId",
    "/devices/{serial}/switch/ports": "portID",
    "/organizations/{organizationId}/admins": "id" 
}


template_schemas = {
    "/networks/{networkId}/wireless/ssids/{number}": {
                    "type": "object",
                    "properties": {
                        "name": {
                            "type": "string",
                            "description": "The name of the SSID"
                        },
                        "enabled": {
                            "type": "boolean",
                            "description": "Whether or not the SSID is enabled"
                        }
                    }
            }
}

def get_product_type(model):
    for product_type, prefixes in PRODUCT_TYPES_MAP.items():
        if any(model.startswith(prefix) for prefix in prefixes):
            return product_type
    return None

def org_admin_handler(path, api_key, base_url, payload, **kwargs):
    event_handler("info", f"Org Admin Handler: {path}")
    #
    # See if we can figure out what this data structure using for its index
    #
    if isinstance(payload, list):
            #
            # First we need to see what data is there to see if we need to put or post
            #
            current_data = meraki_read_path(path, api_key, base_url, **kwargs)
            current_data_by_name = {item['name']: item for item in current_data}
            for item in payload:
                if item["name"] in current_data_by_name:
                    event_handler("info", f"Updating admin user {item['name']}")
                    #
                    # The item exists, so we need to call the api's per-item form
                    #
                    item_path = path + '/{' + index_key + '}'
                    # Use the index_key from the current data instead of the imported data
                    kwargs["id"] = current_data_by_name[item["name"]]["id"]
                    result = meraki_write_path(item_path, args.api_key, base_url, item, api_handler=True, **kwargs)
                else:
                    event_handler("info", f"Adding admin user {item['name']}")
                    #
                    # The item does not exist, so we create it with the same api
                    #
                    result = meraki_write_path(path, args.api_key, base_url, item, api_handler=True, **kwargs)
    else:
        # Cannot be procssed, return the payload
        return (payload)
    return ({})

def default_api_handler(path, api_key, base_url, payload, **kwargs):
    event_handler("info", f"Default handler: {path}")
    result = {}
    index_key = None
    #
    # See if we can figure out what this data structure using for its index
    #
    if isinstance(payload, list):
        #
        # If this is a list, look to see if the index is known
        #
        if path in index_lookup:
            index_key = index_lookup[path]
        if index_key == None:
            event_handler("error", f"Unable to process path {path}: Index not found.")
        else:
            #
            # First we need to see what data is there to see if we need to put or post
            #
            data = meraki_read_path(path, api_key, base_url, **kwargs)
            existing_data_map = {}
            for item in data:
                if "name" in item:
                    existing_data_map[item["name"]] = item
                else:
                    event_handler("error", f"Unable to process path {path}: Name not found.")
                    return ({})
            for item in payload:
                if "radiusServers" in item:
                    for server in item["radiusServers"]:
                        server.pop("serverId", None)
                        server["secret"] = "ChangeMe"
                if "radiusAccountingServers" in item:
                    for server in item["radiusAccountingServers"]:
                        server.pop("serverId", None)
                        server["secret"] = "ChangeMe"  
                if item["name"] in existing_data_map:
                    #
                    # The item exists, so we need to call the api's per-item form
                    #
                    item_path = path + '/{' + index_key + '}'
                    # Use the index_key from the current data instead of the imported data
                    kwargs[index_key] = existing_data_map[item["name"]][index_key]
                    result = meraki_write_path(item_path, args.api_key, base_url, item, **kwargs)
                else:
                    #
                    # The item does not exist, so we create it with the same api
                    #
                    result = meraki_write_path(path, args.api_key, base_url, item, **kwargs)
    else:
        # Cannot be procssed, return the payload
        return (payload)
    return (result)

def noop(path, payload):
    event_handler("warning", f"{path} is currently unsupported")

def wireless_handler(path, api_key, base_url, payload, **kwargs):
    event_handler("info", f"Wireless hander: {path}")
    # if payload["authMode"] == "psk":
    #     payload["psk"] = "ChangeMe"
    if "radiusServers" in payload:
        for server in payload["radiusServers"]:
            server["secret"] = "ChangeMe"
            if "openRoamingCertificateId" in server and server["openRoamingCertificateId"] == None:
                server.pop("openRoamingCertificateId")
    if "radiusAccountingServers" in payload:
        for server in payload["radiusAccountingServers"]:
                server["secret"] = "ChangeMe"
    if "splashUrl" in payload and payload["splashUrl"] == None:
        payload = {}
    return (payload)

def switchport_handler(path, api_key, base_url, payload, **kwargs):
    event_handler("info", f"Switchport hander: {path}")
    for item in payload and item["profile"]["enabled"] == True:
        if "profile" in item:
            # The API does not allow certain attributes when assocaited with a port profile
            if args.ignore_profile:
                item["profile"]["enabled"] = False
            else:
                item.pop("tags", None)
                item.pop("accessPolicyNumber", None)
                item.pop("accessPolicyType", None)
                item.pop("allowedVlans", None)
                item.pop("daiTrusted", None)
                item.pop("name", None)
                item.pop("poeEnabled", None)
                item.pop("rstpEnabled", None)
                item.pop("stpGuard", None)
                item.pop("type", None)
                item.pop("udld", None)
                item.pop("vlan", None)
                item.pop("voiceVlan", None)
                item.pop("isolationEnabled", None)

        item_path = f"{path}/{item['portId']}"
        verb, schema, responses = get_schema(path + "/{portId}", "write", **kwargs)
        kwargs['portId'] = item['portId']
        kwargs['schema'] = schema
        kwargs['verb'] = verb
        result = meraki_write_path(item_path, api_key, base_url, item, **kwargs)
    return (result)

def switch_acl_handler(path, api_key, base_url, payload, **kwargs):
    event_handler("debug", "Called switch_acl_handler")
    new_payload = {
        "rules": []
    }
    #
    # Need to remove the default rule
    #
    for rule in payload["rules"]:
        if rule["comment"] != "Default rule":
            new_payload["rules"].append(rule)
    return (new_payload)

def l3FirewallRules_handler(path, api_key, base_url, payload, **kwargs):
    event_handler("debug", "Called switch_acl_handler")
    new_payload = {
        "rules": [],
        "allowLanAccess": True
    }
    #
    # Need to remove the default rule
    #
    for rule in payload["rules"]:
        if rule["comment"] == "Wireless clients accessing LAN":
            if rule["policy"] == "deny":
                new_payload["allowLanAccess"] = False
        elif rule["comment"] == "Default rule":
            pass
        else:
            new_payload["rules"].append(rule)
    return (new_payload)

api_path_handlers = {
    "/networks/{networkId}/wireless/ssids/{number}": wireless_handler,
    "/networks/{networkId}/wireless/ssids/{number}/splash/settings": wireless_handler,
    "/networks/{networkId}/wireless/ssids/{number}/firewall/l3FirewallRules": l3FirewallRules_handler,
    "/networks/{networkId}/switch/accessControlLists": switch_acl_handler,
    "/organizations/{organizationId}/admins": org_admin_handler

#   "/devices/{serial}/switch/ports": switchport_handler,
#   "/networks/{networkId}/switch/accessPolicies": default_api_handler,
#   "/networks/{networkId}/groupPolicies": default_api_handler,
#   "/networks/{networkId}/appliance/vlans": default_api_handler,
#   "/devices/{serial}/switch/routing/interfaces": default_api_handler,
#   "/networks/{networkId}/vlanProfiles": default_api_handler,
#   "/devices/{serial}/switch/routing/staticRoutes": default_api_handler,
#   "/networks/{networkId}/wireless/ssids/{number}/splash/settings": noop,
#   "/networks/{networkId}/switch/stacks/{switchStackId}/routing/interfaces": default_api_handler,
#   "/networks/{networkId}/switch/stacks/{switchStackId}/routing/staticRoutes": default_api_handler,
#   "/networks/{networkId}/snmp": noop,
#   "/networks/{networkId}/wireless/electronicShelfLabel": noop,
#   "/devices/{serial}/wireless/electronicShelfLabel": noop,
#   "/networks/{networkId}/webhooks/payloadTemplates": noop,
#   "/devices/{serial}/appliance/dhcp/subnets": default_api_handler,
#   "/networks/{networkId}/switch/stacks/{switchStackId}/routing/interfaces/{interfaceId}/dhcp": noop,
#   "/devices/{serial}/switch/routing/interfaces/{interfaceId}/dhcp": noop,
#   "/networks/{networkId}/wireless/rfProfiles": noop,
#   "/networks/{networkId}/wireless/ssids/{number}/splash/settings": noop,
#   "/networks/{networkId}/wireless/ssids/{number}/hotspot20": noop
}

def get_spec_path(path):
    if path in openapi_spec['paths']:
        event_handler("debug", f"Found {path} in spec")
        return path
    else:
        event_handler("debug", f"Finding match for {path} in spec")
        # Sort keys by length in descending order to match the longest path first
        sorted_keys = sorted(index_lookup.keys(), key=len, reverse=True)
        new_path = None
        for key in sorted_keys:
            if path.startswith(key):
                event_handler("debug", f"Found {key} as best match for {path}.")
                # Get the value from index_lookup for the matched key
                index = index_lookup[key]

                # Create a regex pattern to identify the portion to be replaced
                key_with_param = re.sub(r'\{[^}]+\}', '[^/]+', key) + r'/([^/]+)'

                # Replace the first path component after the match with the placeholder value
                new_path =  re.sub(key_with_param, key + r'/{' + index + r'}', path)
                break
        # Return the original path if no match is found
        if new_path:
            return get_spec_path(new_path)
        else:
            event_handler("error", f"Could not find match for {path}")
            return None


def get_schema(spec_path, operation, **kwargs):
    spec_path_data = openapi_spec['paths'][spec_path]
    if operation == 'write':
        if 'put' in spec_path_data:
            verb = 'put'   
            schema = spec_path_data[verb]["requestBody"]["content"]["application/json"]["schema"]
            responses = [int(item) for item in spec_path_data[verb]["responses"].keys()]
        elif 'post' in spec_path_data:
            verb = 'post'
            if "requestBody" in spec_path_data[verb]:
                schema = spec_path_data[verb]["requestBody"]["content"]["application/json"]["schema"]
            else:
                schema = {}
            responses = [int(item) for item in spec_path_data[verb]["responses"].keys()]
        else:
            event_handler("warning", f"{path}, Error: Readonly path")
            verb = None
            schema = {}
    elif operation == 'read':
        if 'get' in spec_path_data:
            verb = 'get'   
            schema = {}
            responses = [int(item) for item in spec_path_data[verb]["responses"].keys()]
        else:
            event_handler("debug", f"{spec_path} is a write-only path")
            verb = None
            schema = {}
            responses = {}
    else:
        event_handler("critical", f"Unknown schema operation {operation}.")
        exit (1)

    if "bound_to_template" in kwargs and kwargs["bound_to_template"] == True:
        #
        # If this is for a template, we override the template schema, but keep the verb
        if spec_path in template_schemas:
            schema = template_schemas[spec_path]
        else:
            schema = {}

    return (verb, schema, responses)

def is_invalid_payload(data, schema):
    try:
        validate(instance=data, schema=schema)
        return None
    except jsonschema.exceptions.ValidationError as err:
        return err.message

def remove_null_values(d):
    # Create a copy of the dictionary to avoid modifying the original during iteration
    keys_to_delete = []

    for key, value in d.items():
        if isinstance(value, dict):
            # Recurse into the nested dictionary
            remove_null_values(value)
            # If the nested dictionary is empty after recursion, mark the key for deletion
            if not value:
                keys_to_delete.append(key)
        elif value is None:
            # Mark keys with null values for deletion
            keys_to_delete.append(key)

    # Remove keys marked for deletion
    for key in keys_to_delete:
        del d[key]

def meraki_request(url, api_key, verb="get", responses=[200], payload={}, parameters=[], **kwargs):
    headers = {
        'Authorization': f'Bearer {api_key}'
    }      
    while True:
        try:
            if verb == "get":
                parameter_string = ""
                for parameter in parameters:
                    if parameter_string == "":
                        parameter_string = '?' + parameter
                    else:
                        parameter_string = parameter_string + '&' + parameter           
                response = requests.get(url + parameter_string,
                        headers =   headers,
                        timeout =   (API_CONNECT_TIMEOUT, API_TRANSMIT_TIMEOUT)
                    )                
            elif verb == "put":
                response = requests.put(url,
                        headers =   headers,
                        json    =   payload,
                        timeout =   (API_CONNECT_TIMEOUT, API_TRANSMIT_TIMEOUT)
                    )
            else:
                response = requests.post(url,
                        headers =   headers,
                        json    =   payload,
                        timeout =   (API_CONNECT_TIMEOUT, API_TRANSMIT_TIMEOUT)
                    )
            
            # Check the status code
            if response.status_code in responses:
                if response.status_code == 204:
                    return {}
                else:
                    return (response.json())
            # elif response.status_code == 401:
            #     event_handler("critical", f"{url}, Error 401: Unauthorized access - check your API key.")
            #     exit (1)
            # elif response.status_code == 404:
            #     event_handler("critical", f"{url}, Error 404: The requested resource was not found.")
            #     exit (1)
            elif response.status_code == 429:
                retry_after = response.headers.get("Retry-After")
                if retry_after is not None:
                    wait_retry = int(retry_after)
                else:
                    wait_retry = API_RETRY_DEFAULT_WAIT             
                event_handler("warning", f"Error 429: Rate limit exceeded. Retrying in {wait_retry} seconds...")
                time.sleep(wait_retry)
            else:
                event_handler("error", f"{url}, Error {response.status_code} ({response.reason}): {response.text}")
                return None
        
        except requests.exceptions.RequestException as e:
            event_handler("critical", f"An error occurred while making the request: {e}")
            exit (1)    

def meraki_read_path(path, api_key, base_url, **kwargs) -> dict | None:
    # Get the spec path that corresponds to this path
    spec_path = get_spec_path(path)
    if spec_path == None:
        event_handler("error", f"Could not find {path} in spec.")
        return (None)
    
    if 'schema' in kwargs:
        schema = kwargs['schema']
        verb = 'get'
    else:
        verb, schema, responses = get_schema(spec_path, "read", **kwargs)
    url = f"{base_url}" + path.format(**kwargs).removesuffix('/')
    if verb == None:
        event_handler("debug", f"{path} is write-only")
        return None
    return meraki_request(url, api_key, responses=responses, **kwargs)

def meraki_write_path(path, api_key, base_url, raw_payload, api_handler=False, **kwargs) -> dict | None:
    change_needed = True
    path = path.removesuffix('/')
    # Get the spec path that corresponds to this path
    spec_path = get_spec_path(path)
    if spec_path == None:
        event_handler("error", f"Could not find {path} in spec.")
        return (None)

    if 'schema' in kwargs:
        schema = kwargs['schema']
        verb = kwargs['verb']
    else:
        verb, schema, responses = get_schema(spec_path, "write", **kwargs)

    full_path = path.format(**kwargs)
    url = f"{base_url}" + full_path

    if "bound_to_template" in kwargs and kwargs["bound_to_template"] == True and schema == {}:
        event_handler("info", f"Network {kwargs['networkId']} is bound to a template. Ignoring {full_path}")
        return {}

    # See if we need a special handler for this path
    if spec_path in api_path_handlers:
        processed_payload = api_path_handlers[spec_path](path, api_key, base_url, raw_payload, **kwargs)
    else:
        processed_payload = raw_payload

    # Reduce the payload down to what is in the schema for the put/post operation 
    filtered_payload = filter_data_by_schema(processed_payload, schema)

    if hasattr(args, 'diff') and args.diff == True and not api_handler:
        show_diff = True
    else:
        show_diff = False

    #
    # Get the current data
    #
    current_data = meraki_read_path(path, args.api_key, base_url, **kwargs)
    if (current_data != None):
        write_only_url = False
        filtered_current_data = filter_data_by_schema(current_data, schema)
        #
        # Diff the current state and the proposed state
        #
        diff_dict = list(diff(filtered_current_data, filtered_payload))
    else:
        write_only_url = True
        diff_dict = []
        event_handler("debug", f"Unable to get path {full_path}")



    # filtered_payload = remove_null_values(filtered_payload)

    if processed_payload and (diff_dict or write_only_url):
        change_needed = True
        if show_diff:
            print(color_message(path.format(**kwargs), "yellow"))
            if not write_only_url:
                # print ("Current:")
                # pprint.pp(filtered_current_data)
                # print ("New:")
                # pprint.pp(filtered_payload)
                # print ("Diff:")
                pprint.pp(diff_dict)
    else:
        change_needed = False
        if show_diff:
            print(color_message(path.format(**kwargs), "green"))

    if args.dry_run or (filtered_payload == {} and schema != {}):
        change_needed = False

    # Override the diff and always write the data
    if hasattr(args, 'always_write') and args.always_write == True:
        change_needed = True

    if change_needed:
        # if args.log_level == "DEBUG":
        return meraki_request(url, api_key, payload=processed_payload, verb=verb, responses=responses, **kwargs)
    else:
        return {}

async def merakiBulkGet(session, path, api_key, base_url, semaphore):
    """
    Fetches the content of the URL using a GET request with headers.

    Parameters:
    - session (aiohttp.ClientSession): The aiohttp session.
    - url (str): The URL to fetch.
    - headers (dict): The headers to include in the request.
    - path (str): The API path to structure the results.
    - semaphore (asyncio.Semaphore): The semaphore to limit concurrent requests.

    Returns:
    - tuple: A tuple containing the path and its response.
    """
    headers = {
        'Authorization': f'Bearer {api_key}'
    }  
    url = f"{base_url}" + path
    retry = 0
    async with semaphore:
        while True:
            try:
                async with session.get(url, headers=headers) as response:
                    if response.status == 200:
                        if response.headers['Content-Type'] == 'application/json':
                            return (path, await response.json())
                        else:
                            return (path, await response.json(encoding="utf-8"))
                    elif response.status == 429:
                        retry_after = response.headers.get("Retry-After")
                     
                        if retry_after is not None:
                            wait_retry = int(retry_after)
                        else:
                            wait_retry = API_RETRY_DEFAULT_WAIT
                        event_handler("debug", f"Received {response.status} status code. Retrying url {url} in {wait_retry} seconds...")
                    elif response.status in [400, 404]:
                        event_handler("error", f"{url}, Error {response.status} ({response.reason})")
                        return (path, {})  
                    else:
                        event_handler("error", f"{url}, Error {response.status} ({response.reason}): {response.text}")
                        return (path, {})
                retry = retry + wait_retry
                await asyncio.sleep(retry)
            except aiohttp.ClientError as e:
                event_handler("error", f"An error occurred: {str(e)}")
            except asyncio.TimeoutError:
                event_handler("error", "The request timed out.")
            except Exception as e:
                event_handler("error", f"An unexpected error occurred: {str(e)}")

async def fetch_all_paths(paths, headers, base_url, max_concurrent_requests):
    """
    Fetches the content of all URLs asynchronously with headers, limiting concurrent requests.

    Parameters:
    - urls (list): A list of tuples containing URLs and paths to fetch.
    - headers (dict): The headers to include in the request.
    - max_concurrent_requests (int): The maximum number of concurrent requests allowed.

    Returns:
    - dict: A dictionary containing all paths and their responses.
    """
    semaphore = asyncio.Semaphore(max_concurrent_requests)
    async with aiohttp.ClientSession() as session:
        tasks = [merakiBulkGet(session, path, headers, base_url, semaphore) for path in paths]
        result_dict = {}
        for task in asyncio.as_completed(tasks):
            # as_completed returns an iterator, so we just have to await the iterator and not call it
            (path, result) = await task
            match = re.match('^/([^/]+)/([^/]+)(/[^{}]*)?$', path)
            if result:
                if match.group(1) not in result_dict:
                    result_dict[match.group(1)] = {}
                if match.group(2) not in result_dict[match.group(1)]:
                    result_dict[match.group(1)][match.group(2)] = {}
                    result_dict[match.group(1)][match.group(2)]['paths'] = {}
                result_dict[match.group(1)][match.group(2)]['paths'][match.group(3)] = result
    return result_dict

def filter_data_by_schema(data, schema):
    """Recursively filters the data to match the structure defined by the schema."""
    if 'properties' in schema:
        filtered_data = {}
        for key, subschema in schema['properties'].items():
            if key in data:
                filtered_data[key] = filter_data_by_schema(data[key], subschema)
        return filtered_data
    elif 'items' in schema and isinstance(data, list):
        return [filter_data_by_schema(item, schema['items']) for item in data]
    else:
        return data

def print_tabular_data(data, columns_to_display, sort_by=None, ascending=True):
    """
    Formats and prints JSON-like data in a tabular format with left-justified headers and data.
    
    Args:
        data (list of dict): The input data to be formatted and printed.
        columns_to_display (list of str): The columns to be printed.
    """
    # Convert the list of dictionaries to a pandas DataFrame
    df = pd.DataFrame(data)

    # Sort the DataFrame by the specified column if provided
    if sort_by:
        df = df.sort_values(by=sort_by, ascending=ascending)

    # Calculate the max width for each column to apply uniform left-justification
    max_col_widths = {col: max(df[col].astype(str).map(len).max(), len(col)) for col in columns_to_display}

    # Left-justify column titles and data with custom formatting
    formatted_rows = []

    # Format the header row
    header_row = '  '.join([col.ljust(max_col_widths[col]) for col in columns_to_display])
    formatted_rows.append(header_row)

    # Format the data rows
    for _, row in df[columns_to_display].iterrows():
        formatted_row = '  '.join([str(row[col]).ljust(max_col_widths[col]) for col in columns_to_display])
        formatted_rows.append(formatted_row)

    # Print the formatted table
    print('\n'.join(formatted_rows))

def print_csv_data(data, columns, sort_by=None, ascending=True):
    # Convert the list of dicts to a DataFrame
    df = pd.DataFrame(data, columns=columns)

    # Alternatively, you can save the CSV to a file
    if args.output_file:
        output_file = args.output_file
        df.to_csv(output_file, index=False)
    else:
        print(df.to_csv(index=False))
          


async def export_command(args):
    #
    # Build the list of paths that we want to export
    #
    api_paths = []
    network_ids = []
    serial_numbers = []
    config_template_ids = []
    stacks_by_network = {}
    parameters = []
    serial_parameters = []
    if args.network_ids:
        network_ids = args.networks
        for network_id in network_ids:
            parameters.append(f"networkIds[]={network_id}")
        networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id, parameters=parameters)
        serial_parameters = parameters
    elif args.tags:
        parameters = ['tagsFilterType=withAnyTags']
        for tag in args.tags:
            parameters.append(f"tags[]={tag}")
        networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id, parameters=parameters)
        for network in networks:
            serial_parameters.append(f"networkIds[]={network['id']}")

    elif args.networks:
        networks = []
        # Create a dict of networks by name
        network_list = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
        networks_by_name = {network['name']: network for network in network_list}
        # Find matches and retrieve their networkId
        for network_name in args.networks:
            for key in networks_by_name.keys():
                if network_name in key:
                    networks.append(networks_by_name[key])
                    network_id = networks_by_name[key].get('id')
                    serial_parameters.append(f"networkIds[]={network_id}")
                    network_ids.append(network_id)

    # if network_ids:
    #     networks = []
    #     template_ids =[]
    #     for network_id in network_ids:
    #         network = meraki_read_path("/networks/{networkId}", args.api_key, args.base_url, networkId=network_id)
    #         networks.append(network)
    #         parameters.append(f"networkIds[]={network_id}")

    #         #
    #         # Collect the config templates that the networks are bound to
    #         #                
    #         if "isBoundToConfigTemplate" in network and network["isBoundToConfigTemplate"] == True:
    #             if network["configTemplateId"] not in template_ids:
    #                 template_ids.append(network["configTemplateId"])
    #     network_ids.extend(template_ids)
    else:
        event_handler("info", f"Exporting all networks for organization {args.org_id}")
        networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
        # List comprehension to extract serial numbers
        network_ids = [item["id"] for item in networks]
        #
        # Get all of the templates for export
        #
        config_template_templates = meraki_read_path("/organizations/{organizationId}/configTemplates", args.api_key, args.base_url, organizationId=args.org_id)
        # List comprehension to extract serial numbers
        network_ids = network_ids + [item["id"] for item in config_template_templates]
    event_handler("info", f"Exporting networks/templates {network_ids}")   
    #
    # Get all of the devices for export
    #
    devices = meraki_read_path("/organizations/{organizationId}/devices", args.api_key, args.base_url, organizationId=args.org_id, parameters=serial_parameters)
    # List comprehension to extract serial numbers
    serial_numbers = [item["serial"] for item in devices]
    for network in networks:
        if "switch" in network["productTypes"]:
            #
            # Get all of the switch stacks for export
            #
            switch_stacks = meraki_read_path("/networks/{networkId}/switch/stacks", args.api_key, args.base_url, networkId=network["id"])
            if switch_stacks:
                stacks_by_network[network["id"]] = {}
                for stack in switch_stacks:
                    switch_stacks_interfaces = meraki_read_path("/networks/{networkId}/switch/stacks/{switchStackId}/routing/interfaces", args.api_key, args.base_url, networkId=network["id"], switchStackId=stack["id"])
                    stack_interface_ids = []
                    for interface in switch_stacks_interfaces:
                        stack_interface_ids.append(interface["interfaceId"])
                    stacks_by_network[network["id"]][stack["id"]] = stack_interface_ids
    #
    # Find all of the paths out of the spec that apply to what we are trying to export
    #
    for path, verbs in openapi_spec['paths'].items():
        # Only get paths that we can write something back to.
        path_exported = False
        if path in config["api_path_exlude"]:
            event_handler("debug", f"{path} is excuded")
            continue
        elif 'get' not in verbs.keys():
            event_handler("debug", f"{path} is write-only")
            continue
        elif (len(verbs) == 1 and "get" in verbs) and not (args.full and path in readonly_exceptions):
            event_handler("debug", f"{path} is read-only")
            continue               
        else:
            get_tags = verbs['get']['tags']
            #
            # /networks/{networkId}: We need to itterate over all of the networks
            #
            if match := re.match('^/networks/{networkId}/?(.*)$', path):
                if args.contexts and not 'networks' in args.contexts:
                    continue
                base_path = "/networks/{networkId}"
                sub_path = "/" + match.group(1)
                for network in networks:
                    network_id = network["id"]
                    # Check to see if this path is approriete for this network
                    if (not 'networks' in get_tags) and (not bool(set(network["productTypes"]) & set(get_tags))):
                        event_handler("debug", f"Skipping {path} for {network['name']}: {network['productTypes']}")
                        continue
                    #
                    # Wireless SSIDS
                    #
                    if match := re.match('^/wireless/ssids/{number}(/.*)?$', sub_path):
                        if match.group(1):
                            wireless_path = match.group(1)
                        else:
                            wireless_path = ""
                        if match := re.match('^/[^{}]+{', wireless_path):
                            # Unhandled paths because they need more dereferencing
                            path_exported = False
                        else:
                            path_exported = True
                            # We need to have one of each of these paths per device
                            for number in range(0, 14):
                                api_paths.append(f"/networks/{network_id}/wireless/ssids/{number}" + wireless_path)
                    #
                    # Switch Stacks
                    # 
                    elif match := re.match('^/switch/stacks/{switchStackId}(/.+)?$', sub_path):
                        if network_id not in stacks_by_network:
                            # There are no stacks
                            continue
                        if match.group(1):
                            stack_path = match.group(1)
                        else:
                            stack_path = ""
                        if match := re.match('^/[^{}]+{', stack_path):
                            # Unhandled paths because they need more dereferencing
                            path_exported = False
                        else:
                            path_exported = True
                            # We need to have one of each of these paths per device
                            for stack_id, stack_interfaces in stacks_by_network[network_id].items():
                                if match := re.match('^/routing/interfaces/{interfaceId}(/.*)?$', stack_path):
                                    if match.group(1):
                                        if match.group(1) == "/dhcp":
                                            for interface_id in stack_interfaces:
                                                api_paths.append(f"/networks/{network_id}/switch/stacks/{stack_id}/routing/interfaces/{interface_id}/dhcp")
                                        else:
                                            api_paths.append(f"/networks/{network_id}/switch/stacks/{stack_id}/routing/interfaces/{interface_id}" + match.group(1))
                                else:
                                    api_paths.append(f"/networks/{network_id}/switch/stacks/{stack_id}" + stack_path)
                    elif match := re.match('^/[^{}]+{', sub_path):
                        # Unhandled paths because they need more dereferencing
                        path_exported = False
                    else:
                        path_exported = True
                        api_paths.append(f"/networks/{network_id}" + sub_path)
            #   
            # /organizations/{args.org_id}
            #
            elif match := re.match('^/organizations/{organizationId}/?(.*)$', path):
                sub_path = '/' + match.group(1)
                if match := re.match('^/[^{}]+{', sub_path):
                    # Unhandled paths because they need more dereferencing
                    path_exported = False
                else:
                    path_exported = True    
                    api_paths.append(f"/organizations/{args.org_id}" + sub_path)
                    
            #
            # /devices/{serial}
            #
            elif match := re.match('^/devices/{serial}/?(.*)$', path):
                if args.contexts and not 'devices' in args.contexts:
                    continue
                sub_path = '/' + match.group(1)
                if match := re.match('^/[^{}]+{', sub_path):
                    # Unhandled paths because they need more dereferencing
                    path_exported = False
                elif devices:                   
                    path_exported = True
                    # We need to have one of each of these paths per device
                    for device in devices:
                        device_type = get_product_type(device["model"])
                        if sub_path == "/" or device_type in get_tags:
                            api_paths.append(f"/devices/{device['serial']}" + sub_path)
            #
            # "/devices/{serial}/switch/routing/interfaces/{interfaceId}"
            #
        if not path_exported:
            event_handler("warning", f"{path} not exported")
    #
    # Make a bulk request to the async function
    #
    result_dict = await fetch_all_paths(api_paths, args.api_key, args.base_url, args.max_concurrent_requests)
    if args.output_file:
        output_file = args.output_file
    else:
        if args.networks and len(args.networks) == 1:
            output_file = f"{args.org_id}_{args.networks[0]}.json"
        else:
            output_file = f"{args.org_id}.json"

    with open(output_file, 'w') as file:
        result_dict['errors'] = error_log
        json.dump(result_dict, file, indent=4)
    event_handler("info", f"Results saved to {output_file}")


def import_file(filename):
        event_handler("info", f"Reading {filename}...")
        try:
            with open(filename, 'r') as file:
                if filename.endswith('json'):
                    return json.load(file)
                else:
                    return yaml.safe_load(file)
        except FileNotFoundError:
            event_handler("critical", f"Error: The file '{args.input_file}' was not found.")
            exit (1)
        except json.JSONDecodeError:
            event_handler("critical", f"Error: The file '{args.input_file}' contains invalid JSON.")
            exit (1)
        except PermissionError:
            event_handler("critical", f"Error: You do not have permission to read the file '{args.input_file}'.")
            exit (1)
        except Exception as e:
            event_handler("critical", f"An unexpected error occurred: {str(e)}")
            exit (1)

def export_file(filename, contents):
    event_handler("info", f"Writing {filename}...")
    with open(filename, 'w') as file:
        json.dump(contents, file, indent=4)

def show_command(args):
    parameters = []

    # For the show commands, we are going to use the meraki python library to get the data
    # because it makes it a bit easeier and we do not have to worry about the API paths
    # not being there.
    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, inherit_logging_config=True, output_log=False)

    if hasattr(args, 'tags') and args.tags:
        for tag in args.tags: 
            parameters.append(f"tags[]={tag}")

    if args.show_command == 'networks':
        # If a data file is provided, use that data instead of making an API call
        if args.input_file:
            import_data = import_file(args.input_file)
            networks = []
            for network_id, network_data in import_data["networks"].items():
                if "/" in network_data["paths"]:
                    networks.append(network_data["paths"]["/"])
        else:
            networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
        if args.json:
            pprint.pp(networks)
        else:
            print_tabular_data(networks, ['name', 'id', 'productTypes', 'timeZone', 'tags'], sort_by='name')      
    elif args.show_command == 'organizations':
        organizations = dashboard.organizations.getOrganizations()
        if args.json:
            pprint.pp(organizations)
        else:
            print_tabular_data(organizations, ['id', 'name'])       
    elif args.show_command == 'devices':
        # If a data file is provided, use that data instead of making an API call
        if args.input_file:
            import_data = import_file(args.input_file)
            source_org_id = next(iter(import_data["organizations"]))
            networks = import_data["organizations"][source_org_id]["paths"]["/networks"]
            networks_by_id = {network['id']: network for network in networks}            
            devices = []
            for serial, device_data in import_data["devices"].items():
                if "/" in device_data["paths"]:
                    device = device_data["paths"]["/"]
                    if device["networkId"] in networks_by_id:
                        device["network"] = networks_by_id[device["networkId"]]["name"]
                    else:
                        device["network"] = device["networkId"]
                    devices.append(device)

        else:
            kwargs = {}
            if args.serial:
                kwargs["serial"] = args.serial
            # devices = meraki_read_path("/organizations/{organizationId}/devices", args.api_key, args.base_url, organizationId=args.org_id)
            devices = dashboard.organizations.getOrganizationDevices(args.org_id, total_pages='all', **kwargs)
            # networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
            networks = dashboard.organizations.getOrganizationNetworks(args.org_id, total_pages='all')
            networks_by_id = {network['id']: network for network in networks}
            # Get the devices status
            devices_statuses = dashboard.organizations.getOrganizationDevicesStatuses(args.org_id, total_pages='all')
            devices_statuses_by_serial = {device_status['serial']: device_status for device_status in devices_statuses}
            #
            # Convert Network ID to Network Name
            #
            for device in devices:
                if device["networkId"] in networks_by_id:
                    device["network"] = networks_by_id[device["networkId"]]["name"]
                else:
                    device["network"] = device["networkId"]
                if device["serial"] in devices_statuses_by_serial:
                    device["status"] = devices_statuses_by_serial[device["serial"]]["status"]
                else:
                    device["status"] = "Unknown"

        if args.json:
            pprint.pp(devices)
        elif args.csv:
            print_csv_data(devices, ['name', 'serial', 'mac', 'model', 'status', 'network', 'tags'], sort_by='network')
        else:
            print_tabular_data(devices, ['name', 'serial', 'mac', 'model', 'status', 'network', 'tags'], sort_by='network')
    elif args.show_command == 'inventory':
        # If a data file is provided, use that data instead of making an API call
        if args.input_file:
            import_data = import_file(args.input_file)
            source_org_id = next(iter(import_data["organizations"]))
            inventory = import_data["organizations"][source_org_id]["paths"]["/inventory/devices"]
        else:
            kwargs = {}
            if args.serials:
                kwargs["serials"] = args.serials
            devices = dashboard.organizations.getOrganizationInventoryDevices(args.org_id, total_pages='all', **kwargs)
            # inventory = meraki_read_path("/organizations/{organizationId}/inventory/devices", args.api_key, args.base_url, organizationId=args.org_id)
            networks = dashboard.organizations.getOrganizationNetworks(args.org_id, total_pages='all')
            networks_by_id = {network['id']: network for network in networks}
            # Get the devices status
            devices_statuses = dashboard.organizations.getOrganizationDevicesStatuses(args.org_id, total_pages='all')
            devices_statuses_by_serial = {device_status['serial']: device_status for device_status in devices_statuses}
            #
            # Convert Network ID to Network Name
            #
            for device in devices:
                if device["networkId"] in networks_by_id:
                    device["network"] = networks_by_id[device["networkId"]]["name"]
                else:
                    device["network"] = device["networkId"]
                if device["serial"] in devices_statuses_by_serial:
                    device["status"] = devices_statuses_by_serial[device["serial"]]["status"]
                else:
                    device["status"] = "Inventory"
        if devices:
            if args.json:
                pprint.pp(devices)
            elif args.csv:
                print_csv_data(devices, ['name', 'serial', 'mac', 'model', 'status', 'network', 'tags'], sort_by='network')
            else:
                print_tabular_data(devices, ['name', 'serial', 'mac', 'model', 'status', 'network', 'tags'], sort_by='network')
    elif args.show_command == 'templates':
        templates = meraki_read_path("/organizations/{organizationId}/configTemplates", args.api_key, args.base_url, organizationId=args.org_id)
        if args.json:
            pprint.pp(templates)
        else:
            print_tabular_data(templates, ['id', 'name', 'productTypes', 'timeZone'])
    elif args.show_command == 'admins':
        # If a data file is provided, use that data instead of making an API call
        if args.input_file:
            import_data = import_file(args.input_file)
            source_org_id = next(iter(import_data["organizations"]))            
            if "/admins" in import_data["organizations"][source_org_id]["paths"]:
                admins = import_data["organizations"][source_org_id]["paths"]["/admins"]
            else:
                admins = []
            networks = []
            # Need to get networks to dereference IDs
            for network_id, network_data in import_data["networks"].items():
                if "/" in network_data["paths"]:
                    networks.append(network_data["paths"]["/"])
        else:
            admins = meraki_read_path("/organizations/{organizationId}/admins", args.api_key, args.base_url, organizationId=args.org_id)
            # Need to get networks to dereference IDs
            networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)


        # Dereference the network IDs to network names
        networks_by_id = {network['id']: network for network in networks}
        for admin in admins:
            for item in admin['networks']:
                if item["id"] in networks_by_id:
                    item["name"] = networks_by_id[item["id"]]["name"]
                    item.pop("id")

        if args.json:
            pprint.pp(admins)
        elif args.csv:
            # Convert the list of dicts to a DataFrame
            df = pd.DataFrame(admins)

            # Print the DataFrame as CSV to the console
            # print(df.to_csv(index=False))

            # Alternatively, you can save the CSV to a file
            if args.output_file:
                output_file = args.output_file
            else:
                output_file = f"{args.org_id}_admins.csv"
            df.to_csv(output_file, index=False)

        else:
            print_tabular_data(admins, ['name', 'email', 'accountStatus', 'hasApiKey', 'lastActive', 'networks'], sort_by='name')      
    elif args.show_command == 'me':
        organizations = meraki_read_path("/organizations", args.api_key, args.base_url)
        me = meraki_read_path("/administered/identities/me", args.api_key, args.base_url)
        org_name = "Unknown"
        for org in organizations:
            if org["id"] == args.org_id:
                org_name = org["name"]

        print (f"Name:            {me['name']}")
        print (f"Email:           {me['email']}")
        print (f"MERAKI_ORG_ID:   {args.org_id} ({org_name})")
        print (f"MERAKI_BASE_URL: {args.base_url}")
    else:
        event_handler("critical", f"Unknown show command {args.show_command}")

def import_command(args):
    #
    # Open the file that we are trying to import
    #
    import_data = import_file(args.input_file)

    if len(import_data["organizations"]) == 0:
        event_handler("critical", "Organization data missing from import file")
        exit (1)
    elif len(import_data["organizations"]) > 1 and args.source_org_id == None:
        event_handler("critical", "Multiple Organizations in import file. `--source-org-id` must be specified.")
        exit (1)
    elif len(import_data["organizations"]) == 1 and args.source_org_id == None:
        source_org_id = next(iter(import_data["organizations"]))
    else:
        source_org_id = args.source_org_id

    if args.import_command == "templates":
        source_config_templates = import_data["organizations"][source_org_id]["paths"]["/configTemplates"]
        target_config_templates = meraki_read_path("/organizations/{organizationId}/configTemplates", args.api_key, args.base_url, organizationId=args.org_id)
        # target_config_template_names = [item["name"] for item in target_config_templates]
        #
        # Create the Templates if the do not already exist
        #
        for source_config_template in source_config_templates:
            if source_config_template["id"] in import_data["networks"]:                
                target_config_template_id = None
                for target_config_template in target_config_templates:
                    if target_config_template["name"] == source_config_template["name"]:
                        event_handler("debug", "Found existing template {source_config_template['name']} ({target_config_template['id']}).")
                        target_config_template_id = target_config_template["id"]
                #
                # If no template was found by that name, it needs to be created
                #
                if target_config_template_id == None:
                    event_handler("info", f"Creating config template {source_config_template['name']}")
                    result = meraki_write_path("/organizations/{organizationId}/configTemplates", args.api_key, args.base_url, source_config_template, organizationId=args.org_id)
                    if result and "id" in result:
                        target_config_template_id = result["id"]
                    else:
                        event_handler("critical", f"Error creating template {source_config_template['name']}")
                        exit (1)
                #
                # Load/Update the template data
                #
                source_config_template_data = import_data["networks"][source_config_template["id"]]["paths"]
                for sub_path, config_template_data in source_config_template_data.items():
                    path = "/networks/{networkId}" + sub_path
                    meraki_write_path(path, args.api_key, args.base_url, config_template_data, networkId=target_config_template_id)
            else:
                event_handler("error", f"Skipping import of template {source_config_template['name']}({source_config_template['id']}) because it is not found in import data.")
    elif args.import_command == "networks":     
        target_networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
        if args.source_network_ids:
            source_network_id_list = args.source_network_ids
        else:
            source_network_id_list = import_data["networks"].keys()
        for source_network_id in source_network_id_list:
            if source_network_id not in import_data["networks"]:
                event_handler("error", f"Skipping import of network {source_network_id} because it is not found in import data.")
                continue
            #
            # Map the source network to the desintation network by name
            #
            bound_to_template = False    
            
            if '/' in import_data["networks"][source_network_id]['paths']:
                source_network_data = import_data["networks"][source_network_id]['paths']['/']
            else:
                event_handler("warning", f"Root for {source_network_id} was not found in import data (Probably a template).")
                continue
            source_network_name = source_network_data['name']     
            target_network_id = None
            for target_network in target_networks:
                if target_network["name"] == source_network_name:
                    target_network_id = target_network["id"]
                    target_network_data = target_network

            if target_network_id == None:
                event_handler("info", f"Creating network {source_network_name}")
                result = meraki_write_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, source_network_data, organizationId=args.org_id)
                if result and "id" in result:
                    target_network = result
                    target_network_data = result
                    target_network_id = target_network["id"]
                else:
                    event_handler("critical", f"Error creating network {source_network_name}")
                    exit (1)

            #
            # Bind the network to the template if present
            #
            if "configTemplateId" in source_network_data:
                bound_to_template = True
                #
                # Find the name of the template associated with the source network's config template
                #
                source_network_template_id = source_network_data["configTemplateId"]
                source_network_template_name = None
                source_network_templates = import_data["organizations"][source_org_id]["paths"]["/configTemplates"]
                for source_network_template in source_network_templates:
                    if source_network_template["id"] == source_network_template_id:
                        source_network_template_name = source_network_template["name"]
                if source_network_template_name == None:
                    event_handler("critical", f"Could not find name for template ID: {source_network_template_id} in org {args.org_id}")
                    exit (1)
                #
                # If we find the name, find the template in the target network with the same name
                #
                target_config_templates = meraki_read_path("/organizations/{organizationId}/configTemplates", args.api_key, args.base_url, organizationId=args.org_id)                    
                target_config_template_id = None
                for target_config_template in target_config_templates:
                    if target_config_template["name"] == source_network_template_name:
                        target_config_template_id = target_config_template["id"]
                if target_config_template_id == None:
                    event_handler("critical", f"Could not find template {source_network_template_name} in org {args.org_id}")
                    exit (1)     

                if "configTemplateId" in target_network_data and target_network_data["configTemplateId"] != target_config_template_id:
                    event_handler("error", f"Target network bound to wrong template ({target_config_template_id} != {target_network['configTemplateId']}). We need to unbind and rebind this template")
                elif "configTemplateId" in target_network_data and target_network_data["configTemplateId"] == target_config_template_id:
                    # No change needed
                    pass
                else:
                    #
                    # Bind the network to the template
                    # 
                    bind_request_payload = {
                        "configTemplateId": target_config_template_id,
                        "autoBind": False
                    }
                    event_handler("debug", f"Binding network {source_network_name} to template {source_network_template_name}")
                    meraki_write_path("/networks/{networkId}/bind", args.api_key, args.base_url, bind_request_payload, networkId=target_network_id)
            if not args.create_only:                                       
                #
                # Load/Update the network data
                #
                kwargs = {}
                kwargs["networkId"] = target_network_id
                kwargs["bound_to_template"] = bound_to_template
                source_network_paths = import_data["networks"][source_network_id]["paths"]
                if "/switch/stacks" in source_network_paths and source_network_paths["/switch/stacks"]:
                    # The source network had switch stacks, so we need to map them to the ones in the new network
                    #
                    # Get all of the switch stacks in the target network for mapping
                    #
                    target_stack_map = {} # by Name
                    source_stack_map = {} # by Id
                    target_switch_stacks = meraki_read_path("/networks/{networkId}/switch/stacks", args.api_key, args.base_url, networkId=target_network_id)

                    for stack in target_switch_stacks:
                        target_stack_map[stack["name"]] = stack
                    for stack in source_network_paths["/switch/stacks"]:
                        if stack["name"] in target_stack_map:
                            source_stack_map[stack["id"]] = target_stack_map[stack["name"]]["id"]
                        else:
                            event_handler("error", f"Mapping for source stack {stack['name']} ({stack['id']}) not found")
                for sub_path, network_data in source_network_paths.items():
                    path = "/networks/{networkId}" + sub_path
                    #
                    # Handle the endpoints for each of the SSIDs
                    #
                    # if wireless_ssid_match := re.match('^/networks/{networkId}/wireless/ssids/([0-9]+)/([^{}]+)$', path):
                    #     kwargs["number"] = wireless_ssid_match.group(1)
                    #     path = "/networks/{networkId}/wireless/ssids/{number}/" + wireless_ssid_match.group(2)
                    #
                    # Handle the Stacks
                    #
                    stack_match = re.match('^/networks/{networkId}/switch/stacks/([^/]+)/?([^{}]*)$', path)
                    if stack_match:
                        source_switchStackId = stack_match.group(1)
                        print (f"Stack: {source_switchStackId}")
                        if source_switchStackId in source_stack_map:
                            kwargs["switchStackId"] = source_stack_map[source_switchStackId]
                            event_handler("debug", f"Mapping source stack: {source_switchStackId} = {kwargs['switchStackId']}")
                        else:
                            event_handler("error", f"Mapping for source stack {source_switchStackId} not found")
                            continue
                        path = "/networks/{networkId}/switch/stacks/{switchStackId}/" + stack_match.group(2)
                    #
                    # Write the data to the path
                    #        
                    meraki_write_path(path, args.api_key, args.base_url, network_data, **kwargs)
    elif args.import_command == "devices":
        for serial in import_data["devices"]:
            # if args.product_types and (sub_path.split('/')[1] in PRODUCT_TYPES and sub_path.split('/')[1] not in args.product_types):
            #     logging.debug(f"Skipping {sub_path}")
            #     continue             
            if args.serials != None and serial not in args.serials:
                event_handler("debug", f"Skipping {serial}")
                continue
            if args.source_network_ids and import_data["devices"][serial]["paths"]['/']["networkId"] not in args.source_network_ids:
                event_handler("debug", f"Skipping {serial} because it is not in source networks list")
                continue
            source_network_paths = import_data["devices"][serial]["paths"]
            for sub_path, device_data in source_network_paths.items():
                path = "/devices/{serial}" + sub_path
                #
                # Write the data to the path
                #
                # Should we remove None values everywhere?
                if "switchProfileId" in device_data and device_data["switchProfileId"] == None:
                    device_data.pop("switchProfileId", None)
                meraki_write_path(path, args.api_key, args.base_url, device_data, serial=serial)
    elif args.import_command == "organizations":
            # Get the key and the value from the dictionary with a single key
            org_import_data = import_data["organizations"]
            key = next(iter(org_import_data))
            source_org_paths = org_import_data[key]["paths"]
            for sub_path, path_data in source_org_paths.items():
                if args.paths and sub_path not in args.paths:
                    event_handler("debug", f"Skipping {sub_path}")
                    continue
                path = "/organizations/{organizationId}" + sub_path
                #
                # Write the data to the path
                #
                meraki_write_path(path, args.api_key, args.base_url, path_data, organizationId=args.org_id)

def claim_command(args):
    claim_list = []

    if args.inventory:
        if args.serials:
            claim_list = args.serials
        elif args.input_file:
            import_data = import_file(args.input_file)
            source_org_id = next(iter(import_data["organizations"]))
            inventory = import_data["organizations"][source_org_id]["paths"]["/inventory/devices"]
            claim_list = [item["serial"] for item in inventory]
        
        if claim_list:
            print (f"Claim {claim_list} into inventory")
            answer = prompt_user(f"Proceed?", ["Yes", "No"])
            if answer == "Yes":
                claim_serials_into_inventory(claim_list)
        else:
            event_handler("info", "No serials to claim")
    else:
        import_data = {}
        if args.input_file:
            import_data = import_file(args.input_file)
        # We always get the list of networks from the API for the target network
        target_network_list = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
        # Create a dict of networks by name
        target_networks_by_name = {network['name']: network for network in target_network_list}
        target_networks_by_id = {network['id']: network for network in target_network_list}

        # First, check to see if the target network ID or name is provided.  It name, need to convert to ID
        target_network_id = None
        if args.network_id:
            target_network_id = args.network_id
        elif args.network:
            # Find matches and retrieve their networkId
            for key in target_networks_by_name.keys():
                if args.network in key:
                    if target_network_id:
                        event_handler("critical", f"Multiple networks found with name {args.network}")
                    target_network_name = target_networks_by_name[key].get('name')
                    target_network_id = target_networks_by_name[key].get('id')

        # now that we have the source and target network IDs, we need to get the list of devices to claim
        if args.serials:
            claim_list = args.serials
            if target_network_id:
                print (f"Claiming {claim_list} into network {target_network_name} ({target_network_id})")
                answer = prompt_user(f"Proceed?", ["Yes", "No"])
                if answer == "Yes":
                    claim_serials_into_network(claim_list, target_network_id)
            else:
                event_handler("critical", "Target network ID must be provided when serials are provided.")
        # Next, check to see if an export file was proved.  If so, we need to get the list of devices from the file
        elif import_data:
            # If an input file is provided, we get the list of devices from the file
            org_keys = list(import_data["organizations"].keys())
            source_network_list = import_data["organizations"][org_keys[0]]["paths"]["/networks"]
            device_by_serial = import_data["devices"]

            # If we have an export file, we need to figure out the network from which to get the devices to claim.
            # If we were given a specific source-network, then then use it.  Otherwise, use the target network name.
            source_network_id = None
            if args.source_network_id:
                source_network_id = args.source_network_id
            elif args.source_network_name:
                # Create a dict of networks by name
                source_networks_by_name = {network['name']: network for network in source_network_list}
                # Find matches and retrieve their networkId
                for key in source_networks_by_name.keys():
                    if args.source_network_name in key:
                        if source_network_id:
                            event_handler("critical", f"Multiple networks found with name {args.source_network_name}")
                        source_network_id = source_networks_by_name[key].get('id')
                        source_network_name = source_networks_by_name[key].get('name')
            elif args.network:
                # Create a dict of networks by name
                source_networks_by_name = {network['name']: network for network in source_network_list}
                # Find matches and retrieve their networkId
                for key in source_networks_by_name.keys():
                    if args.network in key:
                        if source_network_id:
                            event_handler("critical", f"Multiple networks found with name {args.network}")
                        source_network_id = source_networks_by_name[key].get('id')
                        source_network_name = source_networks_by_name[key].get('name')

            if source_network_id:
                event_handler("debug", f"Claiming from network {source_network_id}")
            else:
                claim_by_network = {}
                event_handler("debug", "Claiming all devices in import file.")
                source_networks_by_id = {network['id']: network for network in source_network_list}
                devices = import_data["devices"]
                for device in devices:
                    device_data = devices[device]["paths"]["/"]
                    if device_data["networkId"] in source_networks_by_id:
                        target_network_name = source_networks_by_id[device_data["networkId"]]["name"]
                        event_handler("debug", f"Found {device_data['serial']} in {target_network_name}")
                    else:
                        event_handler("error", "Unable to find source network for device {device_data['serial']}")
                        continue
                    if target_network_name in target_networks_by_name:
                        target_network_id = target_networks_by_name[target_network_name]["id"]
                        event_handler("debug", f"Found Target network ID {target_network_id}")
                    else:
                        event_handler("error", f"Unable to find target network ID for {target_network_name}")
                        continue
                    if target_network_id in claim_by_network:
                        claim_by_network[target_network_id].append(device_data["serial"])
                    else:
                        claim_by_network[target_network_id] = [device_data["serial"]]
                pprint.pp(claim_by_network)
                answer = prompt_user(f"Proceed?", ["Yes", "No"])
                if answer == "Yes":            
                    for target_network_id, serials in claim_by_network.items():
                        event_handler("info", f"Claiming {serials} in network {target_networks_by_id[target_network_id]['name']} ({target_network_id})")
                        claim_serials_into_network(serials, target_network_id)
        else:
            event_handler("critical", "Input file must be provided when serials is not provided.")   


    #     

def claim_serials_into_inventory(serials):
    #
    # See what devices are already claimed in the network
    #
    unclaimed_serials = []
    # Get the current inventory:
    inventory = meraki_read_path("/organizations/{organizationId}/inventory/devices", args.api_key, args.base_url, organizationId=args.org_id)
    inventory_serials = [item["serial"] for item in inventory]
    unclaimed_serials = [serial for serial in serials if serial not in inventory_serials]
    if unclaimed_serials:
        for batch in batch_iterator(unclaimed_serials, args.batch_size):
            claim_payload = {
                "serials": batch
            }
            event_handler("info", f"Claiming {batch} into inventory")
            meraki_write_path("/organizations/{organizationId}/claim", args.api_key, args.base_url, claim_payload, organizationId=args.org_id)
    else:
        event_handler("info", f"No serials to claim in inventory")

def claim_serials_into_network(serials, network_id):
    #
    # See what devices are already claimed in the network
    #
    unclaimed_serials = []
    device_data = meraki_read_path("/networks/{networkId}/devices", args.api_key, args.base_url, networkId=network_id)
    if device_data:
        existing_serials = [device["serial"] for device in device_data]
    else:
        existing_serials = []

    unclaimed_serials = [serial for serial in serials if serial not in existing_serials]
    if unclaimed_serials:
        for batch in batch_iterator(unclaimed_serials, 50):
            claim_payload = {
                "serials": batch,
                "addAtomically": True
            }
            event_handler("debug", f"Claiming {batch} in network {network_id}")
            meraki_write_path("/networks/{networkId}/devices/claim", args.api_key, args.base_url, claim_payload, networkId=network_id)
    else:
        event_handler("debug", f"No serials to claim in network {network_id}")

def unclaim_command(args):
    unclaim_list = []
    import_data = {}

    # First we need to figure out where to get the data that we need, either
    # an input file or the API
    if args.input_file:
        # If an input file is provided, we get the list of devices from the file
        import_data = import_file(args.input_file)
        src_org_id = next(iter(import_data["organizations"]))
        network_list = import_data["organizations"][src_org_id]["paths"]["/networks"]
        device_by_serial = import_data["devices"]
    else:
        # Otherwise we get the list of networks from the API
        network_list = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)

    # Then, check to see if networks are provided.  If the names are provided, we need to get their networkIds
    network_id_list = []
    if args.network_ids:
        network_id_list = args.networks
    elif args.networks:
        # Create a dict of networks by name
        networks_by_name = {network['name']: network for network in network_list}
        # Find matches and retrieve their networkId
        network_id_list = []
        for network_name in args.networks:
            for key in networks_by_name.keys():
                if network_name in key:
                    network_id = networks_by_name[key].get('id')
                    network_id_list.append(network_id)

    if network_id_list:
        event_handler("debug", f"Unclaiming devices from networks {network_id_list}")
        # If we have a list of network IDs, we need to find the devices in those networks to unclaim
        if args.input_file:
            # An input file was provided, so we need to get the devices from the file
            for serial, data in device_by_serial.items():
                device_data = data["paths"]["/"]
                if device_data["networkId"] in network_id_list:
                    unclaim_list.append({"serial": serial, "networkId": device_data["networkId"]})
        else:
            # An input file was not provided, so we need to get the devices from the API
            parameters = []
            # Build the paramater list to limit the devices to the networks provided
            for network_id in network_id_list:
                parameters.append(f"networkIds[]={network_id}")
            # If we have networkIds provided by the user, find the devices in those networks
            device_list = meraki_read_path("/organizations/{organizationId}/devices", args.api_key, args.base_url, organizationId=args.org_id, parameters=parameters)
            if device_list:
                for device in device_list:
                    unclaim_list.append({"serial": device["serial"], "networkId": device["networkId"]})

    elif args.serials:
        # If we were given a list of serial numbers, we need to figure out what network they are in
        if not import_data:
            device_list = meraki_read_path("/organizations/{organizationId}/devices", args.api_key, args.base_url, organizationId=args.org_id)
        devices_by_serial = {device['serial']: device for device in device_list}
        for serial in args.serials:
            if serial in devices_by_serial:
                unclaim_list.append({"serial": serial, "networkId": devices_by_serial[serial]["networkId"]})

    elif args.input_file:
        # If the network_id_list is empty, we need to get all devices in the input file
        for serial, data in device_by_serial.items():
            device_data = data["paths"]["/"]
            unclaim_list.append({"serial": serial, "networkId": device_data["networkId"]})

    if unclaim_list:
        print (unclaim_list)
        answer = prompt_user(f"Unclaim the these devices?", ["Yes", "No"])
        if answer == "Yes":
            for item in unclaim_list:
                event_handler("info", f"Rebooting {item['serial']}.")
                payload = {}
                meraki_write_path("/devices/{serial}/reboot", args.api_key, args.base_url, payload, serial=item['serial'])
                payload = {
                    "serial": item['serial']
                }
                event_handler("info", f"Removing {item['serial']} from network {item['networkId']}")
                meraki_write_path("/networks/{networkId}/devices/remove", args.api_key, args.base_url, payload, networkId=item['networkId'])    
            serials_list = [item["serial"] for item in unclaim_list]
            payload = {
                "serials": serials_list
            }
            event_handler("debug", f"Unclaiming {serials_list} in org {args.org_id}")
            meraki_write_path("/organizations/{organizationId}/inventory/release", args.api_key, args.base_url, payload, organizationId=args.org_id)
    else:
        event_handler("warning", "No devices to unclaim.")

def batch_iterator(lst, batch_size):
    # Generator to yield batches of the specified size
    for i in range(0, len(lst), batch_size):
        yield lst[i:i + batch_size]

def remove_command(args):
    remove_list = []
    import_data = {}

    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, simulate=args.dry_run)

    # First we need to figure out where to get the data that we need, either
    # an input file or the API
    if args.input_file:
        # If an input file is provided, we get the list of devices from the file
        import_data = import_file(args.input_file)
        src_org_id = next(iter(import_data["organizations"]))
        network_list = import_data["organizations"][src_org_id]["paths"]["/networks"]
        device_by_serial = import_data["devices"]
    else:
        # Otherwise we get the list of networks from the API
        # network_list = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
        network_list = dashboard.organizations.getOrganizationNetworks(args.org_id, total_pages='all')

    # Then, check to see if networks are provided.  If the names are provided, we need to get their networkIds
    network_id_list = []
    if args.network_ids:
        network_id_list = args.networks
    elif args.networks:
        # Create a dict of networks by name
        networks_by_name = {network['name']: network for network in network_list}
        # Find matches and retrieve their networkId
        network_id_list = []
        for network_name in args.networks:
            for key in networks_by_name.keys():
                if network_name in key:
                    network_id = networks_by_name[key].get('id')
                    network_id_list.append(network_id)

    if network_id_list:
        event_handler("debug", f"Unclaiming devices from networks {network_id_list}")
        # If we have a list of network IDs, we need to find the devices in those networks to unclaim
        if args.input_file:
            # An input file was provided, so we need to get the devices from the file
            for serial, data in device_by_serial.items():
                device_data = data["paths"]["/"]
                if device_data["networkId"] in network_id_list:
                    remove_list.append({"serial": serial, "networkId": device_data["networkId"]})
        else:
            # An input file was not provided, so we need to get the devices from the API
            parameters = []
            # Build the paramater list to limit the devices to the networks provided
            for network_id in network_id_list:
                parameters.append(f"networkIds[]={network_id}")
            # If we have networkIds provided by the user, find the devices in those networks
            device_list = meraki_read_path("/organizations/{organizationId}/devices", args.api_key, args.base_url, organizationId=args.org_id, parameters=parameters)
            if device_list:
                for device in device_list:
                    remove_list.append({"serial": device["serial"], "networkId": device["networkId"]})

    elif args.serials:
        kwargs = {}
        kwargs["serials"] = args.serials
        # If we were given a list of serial numbers, we need to figure out what network they are in
        if not import_data:
            inventory = dashboard.organizations.getOrganizationInventoryDevices(args.org_id, total_pages='all', **kwargs)
            # device_list = meraki_read_path("/organizations/{organizationId}/devices", args.api_key, args.base_url, organizationId=args.org_id)
            remove_list = inventory
        else:
            devices_by_serial = {device['serial']: device for device in device_list}
            for serial in args.serials:
                if serial in devices_by_serial:
                    remove_list.append({"serial": serial, "networkId": devices_by_serial[serial]["networkId"]})

    elif args.input_file:
        # If the network_id_list is empty, we need to get all devices in the input file
        for serial, data in device_by_serial.items():
            device_data = data["paths"]["/"]
            remove_list.append({"serial": serial, "networkId": device_data["networkId"]})

    if remove_list:
        print (remove_list)
        answer = prompt_user(f"Remove these devices from their networks?", ["Yes", "No"])
        if answer == "Yes":
            for item in remove_list:
                if args.reboot:
                    event_handler("info", f"Rebooting {item['serial']}.")
                    response = dashboard.devices.rebootDevice(item['serial'])
                    event_handler("debug", response)
                # payload = {}
                # meraki_write_path("/devices/{serial}/reboot", args.api_key, args.base_url, payload, serial=item['serial'])
                # payload = {
                #     "serial": item['serial']
                # }
                event_handler("info", f"Removing {item['serial']} from network {item['networkId']}")
                response = dashboard.networks.removeNetworkDevices(item['networkId'], item['serial'])
                event_handler("debug", response)
                # meraki_write_path("/networks/{networkId}/devices/remove", args.api_key, args.base_url, payload, networkId=item['networkId'])    
            # serials_list = [item["serial"] for item in unclaim_list]
            # payload = {
            #     "serials": serials_list
            # }
            # event_handler("debug", f"Unclaiming {serials_list} in org {args.org_id}")
            # meraki_write_path("/organizations/{organizationId}/inventory/release", args.api_key, args.base_url, payload, organizationId=args.org_id)
    else:
        event_handler("warning", "No devices to remove.")

def release_command(args):
    release_list = []

    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, simulate=args.dry_run)

    # If a data file is provided, use that data instead of making an API call
    if args.input_file:
        import_data = import_file(args.input_file)
        source_org_id = next(iter(import_data["organizations"]))
        inventory = import_data["organizations"][source_org_id]["paths"]["/inventory/devices"]
        if args.serials:
            # If we were given a list of serial numbers, we need to make sure they are in the inventory
            for serial in args.serials:
                if serial in inventory_serials:
                    release_list.append(serial)
                else:
                    event_handler("error", f"Serial {serial} not found in inventory.")
            else:
                release_list = [item["serial"] for item in inventory]
    else:
        kwargs = {}
        if args.serials:
            kwargs["serials"] = args.serials
            # inventory = meraki_read_path("/organizations/{organizationId}/inventory/devices", args.api_key, args.base_url, organizationId=args.org_id)
            inventory = dashboard.organizations.getOrganizationInventoryDevices(args.org_id, total_pages='all', **kwargs)
            release_list = [item["serial"] for item in inventory]
        else:
            event_handler("critical", "Input file or serials argument must be provided.")

    if release_list:
        print (release_list)
        answer = prompt_user(f"Release these devices from inventory?", ["Yes", "No"])
        if answer == "Yes":
            response = dashboard.organizations.releaseFromOrganizationInventory(
                args.org_id, 
                serials=release_list
            )
            event_handler("debug", response)
            # # Process the list in batches of 50
            # for batch in batch_iterator(release_list, args.batch_size):
            #     payload = {
            #         "serials": batch
            #     }
            #     event_handler("info", f"Releasing: {batch}")
            #     meraki_write_path("/organizations/{organizationId}/inventory/release", args.api_key, args.base_url, payload, organizationId=args.org_id)    
    else:
        event_handler("warning", "No devices to release.")

def create_command(args):
    # For the show commands, we are going to use the meraki python library to get the data
    # because it makes it a bit easeier and we do not have to worry about the API paths
    # not being there.
    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url)

    if args.create_command == 'networks':
        try:
            response = dashboard.organizations.createOrganizationNetwork(
                args.org_id, args.name, args.product_types,
                tags=args.tags
                # timeZone='America/Los_Angeles', 
                # copyFromNetworkId='N_24329156', 
                # notes='Additional description of the network'
            )
        except meraki.exceptions.APIError as err:
            event_handler("error", err.message['errors'])

def prompt_user(message, options):
    """
    Prompt the user with a message and a list of options.
    
    Args:
        message (str): The prompt message.
        options (list): A list of valid options for the user to choose from.
    
    Returns:
        str: The option chosen by the user.
    """
    options_str = "/".join(options)  # Format the options for display (e.g., Yes/No)
    
    while True:
        user_input = input(f"{message} ({options_str}): ").strip()
        
        if user_input in options:
            return user_input
        else:
            print(f"Invalid input. Please choose one of the following: {options_str}")

def download_spec_command(args):
    parameters = ['version=3']
    spec = meraki_read_path("/organizations/{organizationId}/openapiSpec", args.api_key, args.base_url, parameters=parameters, organizationId=args.org_id)
    export_file(args.output_file, spec)

def event_handler(severity: str, message: str):
    """
    Handle events by logging messages and determining exit behavior based on severity.

    Args:
        message (str): The message to log.
        severity (str): The severity level of the message ('debug', 'info', 'warning', 'error', 'critical').
    """

    severity = severity.lower()
    
    if severity == 'debug':
        logger.debug(message)
    elif severity == 'info':
        logger.info(message)
    elif severity == 'warning':
        logger.warning(message)
        error_log.append(f"{severity}: {message}")
    elif severity == 'error':
        logger.error(message)
        error_log.append(f"{severity}: {message}")
        if args.exit_on_error:
            if args.trace_on_error:
                traceback.print_stack()
            sys.exit(1)
    elif severity == 'critical':
        error_log.append(f"{severity}: {message}")
        logger.critical(message)
        if args.trace_on_error:
            traceback.print_stack()
        sys.exit(1)
    else:
        logger.error(f"Unknown severity level: {severity}")
        if args.exit_on_error:
            sys.exit(1)

# Define log level colors
LOG_COLORS = {
    logging.DEBUG: "\033[0;37m",  # White
    logging.INFO: "\033[0;36m",   # Cyan
    logging.WARNING: "\033[0;33m",# Yellow
    logging.ERROR: "\033[0;31m",  # Red
    logging.CRITICAL: "\033[1;31m" # Bold Red
}

COLORS = {
    "green": "\033[0;32m",
    "yellow": "\033[0;33m",
    "red": "\033[0;31m",
    "blue": "\033[0;34m",
    "reset": "\033[0m"
}

def color_message(message, color):
    return (f"{COLORS.get(color, COLORS['reset'])}{message}{COLORS['reset']}")

class color_formatter(logging.Formatter):
    def format(self, record):
        log_color = LOG_COLORS.get(record.levelno)
        record.msg = f"{log_color}{record.msg}\033[0m"
        return super().format(record)

def setup_logger(level):
    logger = logging.getLogger()
    handler = logging.StreamHandler(sys.stdout)
    formatter = color_formatter('%(levelname)s: %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(level)

def parse_app_args(arguments=None):
    # Create a top-level parser
    parser = argparse.ArgumentParser(description="Meraki Data Tool")
    parser.add_argument('-b', '--base-url', type=str, default=os.getenv('MERAKI_BASE_URL', DEFAULT_MERAKI_BASE_URL), help='The base URL for the API.')
    parser.add_argument('-k', '--api-key', type=str, default=os.getenv('MERAKI_DASHBOARD_API_KEY'), help='The Meraki Organization ID.')
    parser.add_argument('-O', '--org-id', type=str, default=os.getenv('MERAKI_ORG_ID'), help='The organization ID to replace in the paths.')
    parser.add_argument('-c', '--max-concurrent-requests', type=int, default=5, help='The maximum number of concurrent requests allowed.')
    parser.add_argument('--log-level', type=str, default='INFO', choices = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                        help='Set the logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)')
    parser.add_argument('--exit-on-error', action='store_true', help="Exit on error severity.")
    parser.add_argument('--trace-on-error', action='store_true', help="Print stack trace on error.")
    parser.add_argument('--spec-file', default=OPENAPI_SPEC_FILE, type=str, help='The path of the openapi spec file.')


    # Immediately create a subparser holder for a sub-command.
    # The sub-command's string will be stored in "parsed_arguments.cmd"
    #   of the parsed arguments' namespeace object
    subparsers = parser.add_subparsers(dest='command', help='Main commands')

    #
    # Subparser for `download-spec` command
    #
    parser_download_spec = subparsers.add_parser("download-spec", help='Download OpenAPI Spec')
    parser_download_spec.add_argument('-o', '--output_file', type=str, default=OPENAPI_SPEC_FILE, help='The path to write the file.')
    parser_download_spec.set_defaults(func=download_spec_command)

    #
    # Subparser for `export` command
    #
    parser_export = subparsers.add_parser("export", help='Export Data to File')
    parser_export.set_defaults(func=export_command)
    parser_export.add_argument('-o', '--output_file', type=str, help='The path to the output JSON file.')
    parser_export.add_argument('--org-id', help='Org ID', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_export.add_argument('--tags', nargs='+', help='A list of network tags to export.')
    parser_export.add_argument('--networks', nargs='+', help='A list of network IDs to export.')
    parser_export.add_argument('--network_ids', nargs='+', help='A list of network IDs to export.')
    parser_export.add_argument('--full', help='Export all paths', action='store_true')
    parser_export.add_argument('--contexts', nargs='+', default=['organizations','networks','devices'], choices=['organizations','networks','devices', 'inventory'], help='API base paths to import.')


    #
    # Subparser for `import`
    #
    parser_import = subparsers.add_parser("import", help='Import from Export File')
    parser_import.set_defaults(func=import_command)
    import_subparsers = parser_import.add_subparsers(dest='import_command', help='Resources to import')

    # Subcommand: `import organizations`
    parser_import_organizations = import_subparsers.add_parser('organizations', help='Import Organizations')
    parser_import_organizations.add_argument('--source-org-id', help='Source Org ID')
    parser_import_organizations.add_argument('--paths', nargs='+', help='The Paths to import.')
    parser_import_organizations.add_argument('-i', '--input-file', type=str, required=True, help='The path to the output JSON file.')
    parser_import_organizations.add_argument('--dry-run', help='Do name make a change', action='store_true')
    parser_import_organizations.add_argument('--diff', help='Print Diff', action='store_true')
    # Subcommand: `import networks`
    parser_import_networks = import_subparsers.add_parser('networks', help='Import networks')
    parser_import_networks.add_argument('--dry-run', help='Do name make a change', action='store_true')
    parser_import_networks.add_argument('-i', '--input-file', type=str, required=True, help='The path to the output JSON file.')
    parser_import_networks.add_argument('--source-network-ids', nargs='+', help='Source Netork ID')    
    parser_import_networks.add_argument('--diff', help='Print Diff', action='store_true')
    parser_import_networks.add_argument('--always-write', help='Ignore diff and always write to api', action='store_true')
    parser_import_networks.add_argument('--create-only', help='Only create/bind the network', action='store_true')   
    # Subcommand: `import devices`
    parser_import_devices = import_subparsers.add_parser('devices', help='Import devices')
    parser_import_devices.add_argument('-i', '--input-file', type=str, required=True, help='The path to the output JSON file.')
    parser_import_devices.add_argument('--ignore-profile', help='Ignore the profiles', action='store_true')
    parser_import_devices.add_argument('-p', '--product-types', nargs='+', choices=PRODUCT_TYPES, help='The categories to import.')
    parser_import_devices.add_argument('--diff', help='Print Diff', action='store_true')
    parser_import_devices.add_argument('--always-write', help='Ignore diff and always write to api', action='store_true')
    parser_import_devices.add_argument('--dry-run', help='Do name make a change', action='store_true')
    parser_import_devices.add_argument('--source-network-ids', nargs='+', help='List of Source Netork ID')
    parser_import_devices.add_argument('--serials', nargs='+', help='List of Serials')
    # Subcommand: `import templates`
    parser_import_templates = import_subparsers.add_parser('templates', help='Import Templates')
    parser_import_templates.add_argument('--dry-run', help='Do name make a change', action='store_true')
    parser_import_templates.add_argument('-i', '--input-file', type=str, required=True, help='The path to the output JSON file.')
    parser_import_templates.add_argument('--diff', help='Print Diff', action='store_true')
    parser_import_templates.add_argument('--always-write', help='Ignore diff and always write to api', action='store_true')

    #
    # Subparser for `show` command
    #
    parser_show = subparsers.add_parser('show', help='Show resources')
    parser_show.add_argument('--json', help='Show output in JSON', action='store_true')
    parser_show.set_defaults(func=show_command)
    show_subparsers = parser_show.add_subparsers(dest='show_command', help='Resources to show')

    # Subcommand: `show networks`
    parser_show_networks = show_subparsers.add_parser('networks', help='Show networks')
    parser_show_networks.add_argument('--org-id', help='Org ID', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_show_networks.add_argument('--tags', nargs='+', help='Filter by tags')
    parser_show_networks.add_argument('--json', help='Show output in JSON', action='store_true')
    parser_show_networks.add_argument('-i', '--input-file', type=str, help='The path to the output JSON file.')

    # Subcommand: `show organizations`
    parser_show_organizations = show_subparsers.add_parser('organizations', help='Show organizations')

    # Subcommand: `show devices`
    parser_show_devices = show_subparsers.add_parser('devices', help='Show devices')
    parser_show_devices.add_argument('--network', type=str, help='Filter by network ID')
    parser_show_devices.add_argument('--serial', type=str, help='Filter by Serial')
    parser_show_devices.add_argument('--org-id', help='Org ID', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_show_devices.add_argument('-i', '--input-file', type=str, help='The path to the output JSON file.')
    parser_show_devices.add_argument('--json', help='Show output in JSON', action='store_true')
    parser_show_devices.add_argument('--csv', help='Show output in CSV', action='store_true')
    parser_show_devices.add_argument('-o', '--output-file', type=str, default='devices.csv', help='The path to the output file.')

    # Subcommand: `show inventory`
    parser_show_inventory = show_subparsers.add_parser('inventory', help='Show inventory')
    parser_show_inventory.add_argument('--org-id', help='Org ID', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_show_inventory.add_argument('--serials', nargs='+', help='Filter by Serials.')
    parser_show_inventory.add_argument('-i', '--input-file', type=str, help='The path to the output JSON file.')
    parser_show_inventory.add_argument('--json', help='Show output in JSON', action='store_true')
    parser_show_inventory.add_argument('--csv', help='Show output in CSV', action='store_true')
    parser_show_inventory.add_argument('-o', '--output-file', type=str, default='inventory.csv', help='The path to the output file.')

    # Subcommand: `show templates`
    parser_show_templates = show_subparsers.add_parser('templates', help='Show templates')
    parser_show_templates.add_argument('--org-id', help='Org ID', default=os.getenv('MERAKI_ORG_ID'), type=str)

    # Subcommand: `show me`
    parser_show_templates = show_subparsers.add_parser('me', help='Show Current User')
    parser_show_templates.add_argument('--org-id', help='Org ID', default=os.getenv('MERAKI_ORG_ID'), type=str)    

    # Subcommand: `show admins`
    parser_show_admins = show_subparsers.add_parser('admins', help='Show Current User')
    parser_show_admins.add_argument('-i', '--input-file', type=str, help='The path to the output JSON file.')
    parser_show_admins.add_argument('--json', help='Show output in JSON', action='store_true')
    parser_show_admins.add_argument('--csv', help='Show output in CSV', action='store_true')
    parser_show_admins.add_argument('-o', '--output_file', type=str, help='The path to the output file.')


    parser_import.add_argument('--source-network-id', help='Source Netork ID', type=str)
    parser_import.add_argument('--source-org-id', help='Source Org ID', type=str)

    #
    # Subparser for `create` command
    #
    parser_create = subparsers.add_parser('create', help='Create Resources')
    parser_create.set_defaults(func=create_command)
    create_subparsers = parser_create.add_subparsers(dest='create_command', help='Create Resources')

    # Subcommand: `create networks`
    parser_create_newtorks = create_subparsers.add_parser('networks', help='Create Networks')
    parser_create_newtorks.add_argument('--org-id', help='Org ID', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_create_newtorks.add_argument('--name', help='Network Name', required=True, type=str)
    parser_create_newtorks.add_argument('--product-types', nargs='+', default=['appliance', 'switch', 'wireless'], help="Product Types")
    parser_create_newtorks.add_argument('--tags', nargs='+', default=[], help="Tags")

    #
    # Subparser for `release` command
    #
    parser_release = subparsers.add_parser('release', help='Release from Inventory')
    parser_release.set_defaults(func=release_command)
    parser_release.add_argument('-i', '--input-file', type=str, help='The path to the output JSON file.')
    parser_release.add_argument('--dry-run', help='Do name make a change', action='store_true')
    parser_release.add_argument('--serials', nargs='+', help='The serial numbers to unclaim.')

    #
    # Subparser for `remove` command
    #
    parser_remove = subparsers.add_parser('remove', help='Remove Devices')
    parser_remove.set_defaults(func=remove_command)
    parser_remove.add_argument('-i', '--input-file', type=str, help='The path to the output JSON file.')
    parser_remove.add_argument('--dry-run', help='Simulate the Change', action='store_true')
    parser_remove.add_argument('--reboot', help='Reboot Before Removal', action='store_true')
    parser_remove_group = parser_remove.add_mutually_exclusive_group()
    parser_remove_group.add_argument('--network-ids', help='Source Netork IDs', type=str)
    parser_remove_group.add_argument('--networks', nargs='+', help='Source Netorks')
    parser_remove_group.add_argument('--serials', nargs='+', help='The serial numbers to unclaim.')

    #
    # Subparser for `unclaim` command
    #
    parser_unclaim = subparsers.add_parser('unclaim', help='Unclaim Devices')
    parser_unclaim.set_defaults(func=unclaim_command)
    parser_unclaim.add_argument('-i', '--input-file', type=str, help='The path to the output JSON file.')
    parser_unclaim.add_argument('--dry-run', help='Do name make a change', action='store_true')
    parser_unclaim_group = parser_unclaim.add_mutually_exclusive_group()
    parser_unclaim_group.add_argument('--network-ids', help='Source Netork IDs', type=str)
    parser_unclaim_group.add_argument('--networks', nargs='+', help='Source Netorks')
    parser_unclaim_group.add_argument('--serials', nargs='+', help='The serial numbers to unclaim.')

    #
    # Subparser for `claim` command
    #
    parser_claim = subparsers.add_parser("claim", help='Claim Devices')
    parser_claim.set_defaults(func=claim_command)
    parser_claim.add_argument('-i', '--input-file', type=str, help='The path to the output JSON file.')
    parser_claim.add_argument('--dry-run', help='Do name make a change', action='store_true')
    parser_claim.add_argument('--network-id', help='Target Network ID', type=str)
    parser_claim.add_argument('--network', help='Target Network Name', type=str)
    parser_claim.add_argument('--source-network-id', help='Source Network ID', type=str)
    parser_claim.add_argument('--source-network-name', help='Source Network Name', type=str)    
    parser_claim.add_argument('--serials', nargs='+', help='The serial numbers to import.')
    parser_claim.add_argument('--inventory', help='Clain into inventory', action='store_true')
    parser_claim.add_argument('--batch-size', help='Batch Size', default=50, type=int)


    return parser.parse_args(arguments)    

async def main(args):
    # Execute the function for the subcommand
    if args.command =='export':
        await export_command(args)
    else:
        if hasattr(args, 'func'):
            args.func(args)
    # else:
    #     parser.print_help()

if __name__ == "__main__":
    suppress_logging = False
    output_log = False
    log_path = os.path.join(os.getcwd(), "log")
    if not os.path.exists(log_path):
        os.makedirs(log_path)

    error_log = []
    args = parse_app_args()
    log_level = getattr(logging, args.log_level.upper(), logging.INFO)
    setup_logger(log_level)
    logger = logging.getLogger()

    config_file = os.path.dirname(os.path.realpath(__file__)) + '/msak.yaml'
    if os.path.exists(config_file):
        config = import_file(config_file)
    
    openapi_spec = import_file(args.spec_file)
    logger.debug(args)

    asyncio.run(main(args))

#!/usr/bin/env python3
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

import aiohttp
import asyncio
import json
import argparse
import os
import yaml
import logging
import re
import requests
import time
import pprint
import sys
import traceback
from dictdiffer import diff
import jsonschema
from jsonschema import validate
from aiohttp.client_exceptions import ClientError
import pandas as pd
import meraki

DEFAULT_MERAKI_BASE_URL = 'https://api.meraki.com/api/v1'
CONFIG_FILES = ['/etc/meraki/meraki_inventory.yml', '/etc/ansible/meraki_inventory.yml']
OPENAPI_SPEC_FILE = 'meraki-openapi-spec.json'
API_MAX_RETRIES             = 3
API_CONNECT_TIMEOUT         = 60
API_TRANSMIT_TIMEOUT        = 60
API_STATUS_RATE_LIMIT       = 429
API_RETRY_DEFAULT_WAIT      = 3

import_data = {}

EXPECTED_ERROR_PHRASES = [
    "unsupported",
    "only supports",
    "not enabled",
    "not supported",
    "does not contain",
    "does not support"
]

def expected_error_message(string):
    """
    Check if the provided string contains any known expected error phrases.

    Args:
        string (str): The string to check for error phrases.

    Returns:
        bool: True if an expected error phrase is found, False otherwise.
    """
    if isinstance(string, str):
        string_lower = string.lower()
        return any(phrase.lower() in string_lower for phrase in EXPECTED_ERROR_PHRASES)
    return False

PRODUCT_TYPES = [
        'appliance',
        'switch',
        'wireless',
        'statuses',
        'sensor',
        'camera',
        'cellularGateway',
        'categories'
]

PRODUCT_TYPES_MAP = {
    'appliance': ['MX'],
    'wireless': ['MR', 'CW'],
    'switch': ['MS']
}

DIFF_IGNORE_KEYS = [
    'psk',
    'billing.replyToEmailAddress',
    'billing.prepaidAccessFastLoginEnabled'
]

SKIP_TAGS = [
    # 'monitor',
    'liveTools',
    'cableTest',
    'ping',
    'apiRequests',
    'firmware',
    'upgrades',
    'packetLoss',
    'byNetwork',
    'byDevice',
    'openapiSpec',
    'sm',
    'insight',
    'monitoredMediaServers',
    'applicationCategories'
]

readonly_exceptions = [
    "/organizations/{organizationId}/inventory/devices",
    "/devices/{serial}/switch/ports",
    "/networks/{networkId}/appliance/ports"
]

index_lookup = {
    "/networks/{networkId}/switch/accessPolicies": "accessPolicyNumber",
    "/networks/{networkId}/wireless/ssids": "number",
    "/networks/{networkId}/groupPolicies": "groupPolicyId",
    "/networks/{networkId}/appliance/vlans": "vlanId",
    "/devices/{serial}/switch/routing/interfaces": "interfaceId",
    "/networks/{networkId}/switch/stacks/{switchStackId}/routing/interfaces": "interfaceId",
    "/networks/{networkId}/vlanProfiles": "iname",
    "/devices/{serial}/switch/routing/staticRoutes": "staticRouteId",
    "/networks/{networkId}/switch/stacks/{switchStackId}/routing/staticRoutes": "staticRouteId",
    "/devices/{serial}/switch/ports": "portID",
    "/organizations/{organizationId}/admins": "id"
}

key_lookup_map = {
    "/organizations/{organizationId}/adaptivePolicy/groups": "groupId",
    "/networks/{networkId}/appliance/ports": "number"
}

def key_lookup(path):
    if path in key_lookup_map:
        return key_lookup_map[path]
    else:
        return None

template_schemas = {
    "/networks/{networkId}/wireless/ssids/{number}": {
                    "type": "object",
                    "properties": {
                        "name": {
                            "type": "string",
                            "description": "The name of the SSID"
                        },
                        "enabled": {
                            "type": "boolean",
                            "description": "Whether or not the SSID is enabled"
                        }
                    }
            }
}

def get_product_type(model):
    """
    Determine the product type (appliance, wireless, switch) based on the model prefix.

    Args:
        model (str): The model string to check.

    Returns:
        str or None: The product type if matched, otherwise None.
    """
    for product_type, prefixes in PRODUCT_TYPES_MAP.items():
        if any(model.startswith(prefix) for prefix in prefixes):
            return product_type
    return None

# def org_admin_handler(path, api_key, base_url, payload, **kwargs):
#     event_handler("info", f"Org Admin Handler: {path}")
#     #
#     # See if we can figure out what this data structure using for its index
#     #
#     if isinstance(payload, list):
#             #
#             # First we need to see what data is there to see if we need to put or post
#             #
#             current_data = meraki_read_path(path, api_key, base_url, **kwargs)
#             current_data_by_name = {item['name']: item for item in current_data}
#             for item in payload:
#                 if item["name"] in current_data_by_name:
#                     event_handler("info", f"Updating admin user {item['name']}")
#                     #
#                     # The item exists, so we need to call the api's per-item form
#                     #
#                     item_path = path + '/{' + source_key + '}'
#                     # Use the source_key from the current data instead of the imported data
#                     kwargs["id"] = current_data_by_name[item["name"]]["id"]
#                     result = meraki_write_path(item_path, args.api_key, base_url, item, api_handler=True, **kwargs)
#                 else:
#                     event_handler("info", f"Adding admin user {item['name']}")
#                     #
#                     # The item does not exist, so we create it with the same api
#                     #
#                     result = meraki_write_path(path, args.api_key, base_url, item, api_handler=True, **kwargs)
#     else:
#         # Cannot be procssed, return the payload
#         return (payload)
#     return ({})

# def noop(path, payload):
#     event_handler("warning", f"{path} is currently unsupported")

def wireless_handler(path, api_key, base_url, payload, **kwargs) -> dict | None:
    """
    Handle wireless SSID payloads, masking secrets and removing unsupported fields.

    Args:
        path (str): API path.
        api_key (str): Meraki API key.
        base_url (str): Meraki API base URL.
        payload (dict): The payload to process.
        **kwargs: Additional arguments.

    Returns:
        dict or None: The processed payload, or None if not applicable.
    """
    event_handler("info", f"Wireless hander: {path}")
    # if payload["authMode"] == "psk":
    #     payload["psk"] = "ChangeMe"
    if "radiusServers" in payload:
        for server in payload["radiusServers"]:
            server["secret"] = "ChangeMe"
            # if "openRoamingCertificateId" in server and server["openRoamingCertificateId"] == None:
            #     server.pop("openRoamingCertificateId")
    if "radiusAccountingServers" in payload:
        for server in payload["radiusAccountingServers"]:
                server["secret"] = "ChangeMe"
    if "splashLogo" in payload and payload["splashLogo"]:
        # Going to just load without the logo for now.
        payload.pop("splashLogo")
    return (payload)

# def switchport_handler(path, api_key, base_url, payload, **kwargs):
#     event_handler("info", f"Switchport hander: {path}")
#     for item in payload and item["profile"]["enabled"] == True:
#         if "profile" in item:
#             # The API does not allow certain attributes when assocaited with a port profile
#             if args.ignore_profile:
#                 item["profile"]["enabled"] = False
#             else:
#                 item.pop("tags", None)
#                 item.pop("accessPolicyNumber", None)
#                 item.pop("accessPolicyType", None)
#                 item.pop("allowedVlans", None)
#                 item.pop("daiTrusted", None)
#                 item.pop("name", None)
#                 item.pop("poeEnabled", None)
#                 item.pop("rstpEnabled", None)
#                 item.pop("stpGuard", None)
#                 item.pop("type", None)
#                 item.pop("udld", None)
#                 item.pop("vlan", None)
#                 item.pop("voiceVlan", None)
#                 item.pop("isolationEnabled", None)

#         item_path = f"{path}/{item['portId']}"
#         verb, schema, responses = get_schema(path + "/{portId}", "write", **kwargs)
#         kwargs['portId'] = item['portId']
#         kwargs['schema'] = schema
#         kwargs['verb'] = verb
#         result = meraki_write_path(item_path, api_key, base_url, item, **kwargs)
#     return (result)

def switch_acl_handler(path, api_key, base_url, payload, **kwargs) -> dict | None:
    """
    Process switch ACL payloads, removing the default rule.

    Args:
        path (str): API path.
        api_key (str): Meraki API key.
        base_url (str): Meraki API base URL.
        payload (dict): The payload to process.
        **kwargs: Additional arguments.

    Returns:
        dict: The processed payload with the default rule removed.
    """
    event_handler("debug", "Called switch_acl_handler")
    new_payload = {
        "rules": []
    }
    #
    # Need to remove the default rule
    #
    for rule in payload["rules"]:
        if rule["comment"] != "Default rule":
            new_payload["rules"].append(rule)
    return (new_payload)

def appliance_vlan_handler(path, api_key, base_url, payload, **kwargs) -> dict | None:
    """
    Process appliance VLAN payloads, handling template-specific fields and type corrections.

    Args:
        path (str): API path.
        api_key (str): Meraki API key.
        base_url (str): Meraki API base URL.
        payload (dict): The payload to process.
        **kwargs: Additional arguments (e.g., is_template).

    Returns:
        dict or None: The processed payload, or None if invalid input.
    """
    event_handler("debug", "Called appliace_vlan_handler")
    if isinstance(payload, dict):
        new_payload = payload
        if "is_template" in kwargs and kwargs["is_template"] == True:
            new_payload.pop("ipv6", None)
        if "id" in new_payload and isinstance(new_payload["id"], int):
            new_payload["id"] = str(new_payload["id"])
    else:
        event_handler("warning", f"Invalid payload (list) for POST with {path}")
        return (None)
    return (new_payload)

def l3FirewallRules_handler(path, api_key, base_url, payload, **kwargs) -> dict | None:
    """
    Process L3 firewall rules payload, handling policy object translation and removing default rules.

    Args:
        path (str): API path.
        api_key (str): Meraki API key.
        base_url (str): Meraki API base URL.
        payload (dict): The payload to process.
        **kwargs: Additional arguments.

    Returns:
        dict: The processed payload with translated policy objects and filtered rules.
    """
    event_handler("debug", "Called switch_acl_handler")
    new_payload = {
        "rules": [],
        "allowLanAccess": True
    }
    def translate_policy_objects(rule):
        if 'GRP' in rule or 'OBJ' in rule:
            parts = rule.split(',')
            new_parts = []
            for part in parts:
                if match := re.search(r'GRP\((\d+)\)', part):
                    group_id = match.group(1)
                    if new_group_id := translate_path("/organizations/{organizationId}/policyObjects/groups/" + group_id):
                        new_parts.append(f"GRP({new_group_id})")
                elif match := re.search(r'OBJ\((\d+)\)', part):
                    object_id = match.group(1)
                    if new_object_id := translate_path("/organizations/{organizationId}/policyObjects/" + object_id):
                        new_parts.append(f"OBJ({int(new_object_id)})")
                else:
                    new_parts.append(part)
            return ','.join(new_parts)
        return None
    #
    # Need to remove the default rule
    #
    for rule in payload["rules"]:
        if rule["comment"] == "Wireless clients accessing LAN":
            if rule["policy"] == "deny":
                new_payload["allowLanAccess"] = False
        elif rule["comment"] == "Default rule":
            # Drop the default rule.
            pass
        else:
            # The rule should be added, but might need to be modified.
            if "srcCidr" in rule:
                if new_item := translate_policy_objects(rule["srcCidr"]):
                    rule["srcCidr"] = new_item
            if "destCidr" in rule:
                if new_item := translate_policy_objects(rule["destCidr"]):
                    rule["destCidr"] = new_item
            new_payload["rules"].append(rule)
    return (new_payload)

def content_policy_handler(path, api_key, base_url, payload, **kwargs) -> dict | None:
    """
    Process content filtering policy payload, converting blocked URL categories to IDs.

    Args:
        path (str): API path.
        api_key (str): Meraki API key.
        base_url (str): Meraki API base URL.
        payload (dict): The payload to process.
        **kwargs: Additional arguments.

    Returns:
        dict: The processed payload with converted blockedUrlCategories.
    """
    event_handler("debug", f"Called content_policy_handler on path {path}")
    if "blockedUrlCategories" in payload:
        converted_categories = []
        for category in payload["blockedUrlCategories"]:
            converted_categories.append(category["id"])
        payload["blockedUrlCategories"] = converted_categories
    return(payload)
        
def policy_object_handler(path, api_key, base_url, payload, **kwargs) -> dict | None:
    """
    Process policy object payload, translating group and object IDs and removing networkIds.

    Args:
        path (str): API path.
        api_key (str): Meraki API key.
        base_url (str): Meraki API base URL.
        payload (dict): The payload to process.
        **kwargs: Additional arguments.

    Returns:
        dict: The processed payload with translated IDs and cleaned fields.
    """
    event_handler("debug", f"Called policy_object_handler on path {path}")
    if "groupIds" in payload:
        new_group_ids = []
        for group_id in payload["groupIds"]:
            if new_group_id := translate_path(f"/organizations/{{kwargs['organizationId']}}/policyObjects/groups/{{group_id}}"):
                new_group_ids.append(new_group_id)
        payload["groupIds"] = new_group_ids
    if "networkIds" in payload:
        payload.pop("networkIds")
    if "objectIds" in payload:
        new_object_ids = []
        for object_id in payload["objectIds"]:
            if new_object_id := translate_path(f"/organizations/{{kwargs['organizationId']}}/policyObjects/{{object_id}}"):
                new_object_ids.append(int(new_object_id))
        payload["objectIds"] = new_object_ids
    return (payload)

def device_handler(path, api_key, base_url, payload, **kwargs) -> dict | None:
    """
    Process device payloads, translating floorPlanId if necessary.

    Args:
        path (str): API path.
        api_key (str): Meraki API key.
        base_url (str): Meraki API base URL.
        payload (dict): The payload to process.
        **kwargs: Additional arguments.

    Returns:
        dict: The processed payload with translated floorPlanId if applicable.
    """
    event_handler("debug", f"Called device_handler on path {path}")
    #
    # This is REALLY inefficent.  Need to fix later.
    #
    if "floorPlanId" in payload:
        floorplan_id = payload["floorPlanId"]
        #
        # The device needs to be in a network with a floorplan
        #
        if "networkId" in payload and payload["networkId"]:
            network_name = None
            floorplan_name = None
            target_network_id = None
            target_floorplan_id = None
            network_id = payload["networkId"]
            import_floorplan_data = get_from_import(f"/networks/{network_id}/floorPlans")
            #
            # Get the name of the floorplan from the import
            #
            if import_floorplan_data:
                for floorplan in import_floorplan_data:
                    if floorplan["floorPlanId"] == floorplan_id:
                        floorplan_name = floorplan["name"]
                if floorplan_name:
                    import_network_data = get_from_import("/organizations/{organizationId}/networks")
                    for network in import_network_data:
                        if network["id"] == network_id:
                            network_name = network["name"]
                    if network_name:
                        #
                        # Get the name of the floorplan from the current instance
                        #
                        dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, simulate=args.dry_run, output_log=False)
                        target_networks = dashboard.organizations.getOrganizationNetworks(args.org_id, total_pages='all')
                        for network in target_networks:
                            if network["name"] == network_name:
                                target_network_id = network["id"]
                        if target_network_id:
                            target_floorplans = dashboard.networks.getNetworkFloorPlans(target_network_id)
                            for floorplan in target_floorplans:
                                if floorplan["name"] == floorplan_name:
                                    target_floorplan_id = floorplan["floorPlanId"]
                        if target_floorplan_id:
                            payload["floorPlanId"] = target_floorplan_id
                            event_handler("debug",f"Translated floorplan {floorplan_name} from {floorplan_id} to {target_floorplan_id}")
                        else:
                            event_handler("error", f"Could not find floorplan {floorplan_name} in current instance")
            else:
                event_handler("error", f"Could not find floorplan {floorplan_id} in import data")
    return (payload)
#
# This data structure is used to map the API paths to the correct API handler, as well
# as the subpath and the key used for uniquness.
#
api_data_handlers = {
    "/networks/{networkId}/switch/accessControlLists": switch_acl_handler,
    "/networks/{networkId}/appliance/vlans": appliance_vlan_handler,
    "/networks/{networkId}/appliance/vlans/{vlanId}": appliance_vlan_handler,
    "/networks/{networkId}/wireless/ssids/{number}/splash/settings": wireless_handler,
    "/networks/{networkId}/wireless/ssids/{number}/firewall/l3FirewallRules": l3FirewallRules_handler,
    "/organizations/{organizationId}/policyObjects": policy_object_handler,
    "/organizations/{organizationId}/policyObjects/{policyObjectId}" : policy_object_handler,
    "/organizations/{organizationId}/policyObjects/groups/{policyObjectGroupId}": policy_object_handler,
    "/networks/{networkId}/appliance/firewall/l3FirewallRules": l3FirewallRules_handler,
    "/networks/{networkId}/appliance/contentFiltering": content_policy_handler,
    "/devices/{serial}": device_handler
}

api_path_handlers = {
    "/networks/{networkId}/appliance/vlans/{vlanId}": ["id", "id"],
    "/networks/{networkId}/groupPolicies/{groupPolicyId}": ["name", "groupPolicyId"],
    "/networks/{networkId}/floorPlans/{floorPlanId}": ["name", "floorPlanId"],
    "/organizations/{organizationId}/policyObjects/{policyObjectId}": ["name", "id"],
    "/organizations/{organizationId}/policyObjects/groups/{policyObjectGroupId}": ["name", "id"]
    # "/networks/{networkId}/wireless/ssids/{number}/firewall/l3FirewallRules": l3FirewallRules_handler,
    # "/networks/{networkId}/wireless/ssids/{number}": wireless_handler,
    # "/networks/{networkId}/wireless/ssids/{number}/splash/settings": wireless_handler,
    # "/networks/{networkId}/switch/accessControlLists": switch_acl_handler,
    # "/organizations/{organizationId}/admins": org_admin_handler,
    # "/devices/{serial}/switch/ports": switchport_handler
#   "/networks/{networkId}/switch/accessPolicies": default_api_handler,
#   "/networks/{networkId}/groupPolicies": default_api_handler,
#   "/networks/{networkId}/appliance/vlans": default_api_handler,
#   "/devices/{serial}/switch/routing/interfaces": default_api_handler,
#   "/networks/{networkId}/vlanProfiles": default_api_handler,
#   "/devices/{serial}/switch/routing/staticRoutes": default_api_handler,
#   "/networks/{networkId}/wireless/ssids/{number}/splash/settings": noop,
#   "/networks/{networkId}/switch/stacks/{switchStackId}/routing/interfaces": default_api_handler,
#   "/networks/{networkId}/switch/stacks/{switchStackId}/routing/staticRoutes": default_api_handler,
#   "/networks/{networkId}/snmp": noop,
#   "/networks/{networkId}/wireless/electronicShelfLabel": noop,
#   "/devices/{serial}/wireless/electronicShelfLabel": noop,
#   "/networks/{networkId}/webhooks/payloadTemplates": noop,
#   "/devices/{serial}/appliance/dhcp/subnets": default_api_handler,
#   "/networks/{networkId}/switch/stacks/{switchStackId}/routing/interfaces/{interfaceId}/dhcp": noop,
#   "/devices/{serial}/switch/routing/interfaces/{interfaceId}/dhcp": noop,
#   "/networks/{networkId}/wireless/rfProfiles": noop,
#   "/networks/{networkId}/wireless/ssids/{number}/splash/settings": noop,
#   "/networks/{networkId}/wireless/ssids/{number}/hotspot20": noop
}

def get_last_placeholder(path: str) -> str or None:
    """
    Splits the given path by '/' and checks if the last non-empty
    component is a placeholder in braces. If so, returns the
    placeholder without braces; else returns None.
    
    Examples:
        "/organizations/{organizationId}/policyObjects/groups/{policyObjectGroupId}"
          -> "policyObjectGroupId"
        "/networks/{networkId}/wireless/ssids/{number}/firewall/l3FirewallRules"
          -> None
    """
    # Split the path into components, ignoring empty ones
    components = [comp for comp in path.split('/') if comp]
    if not components:
        return None

    last_comp = components[-1]
    if last_comp.startswith('{') and last_comp.endswith('}'):
        # Return the placeholder without braces
        return last_comp[1:-1]
    else:
        return None

def get_spec_path(path):
    context, context_item, subpath = parse_path(path)
    if not context_item:
        path = f"/{context}"
    else:
        if context_item.startswith('{'):
            # The contect_item is already a placeholder
            placeholder = context_item
        else:
            # We need to figure out the correct placeholder from the context
            if context == "organizations":
                placeholder = "{organizationId}"
            elif context == "networks":
                placeholder = "{networkId}"
            elif context == "devices":
                placeholder = "{serial}"
            elif context == "administered":
                placeholder = context_item
            else:
                event_handler("error", f"Unknown path {path} in spec")
                return None    
        if context:
            path = f"/{context}/{placeholder}"
            if subpath and subpath != "/":
                path = path + subpath
        else:
            event_handler("error", f"Unknown path {path} in spec")
            return None
    if path in openapi_spec['paths']:
        # event_handler("debug", f"Found {path} in spec")
        return path
    else:
        #
        # Need to do it the hard way by looking through the spec
        #
        event_handler("debug", f"Finding match for {path} in spec")
        for key in openapi_spec['paths'].keys():
            if key.startswith(f"/{context}/"):
                pattern = re.sub(r"\{[^/]+\}", r"[^/]+", key)
                regex_pattern = f"^{pattern}$"
                compiled_regex = re.compile(regex_pattern)
                if bool(compiled_regex.match(path)):
                    event_handler("debug", f"Found {key} as best match for {path}.")
                    return key
    event_handler("error", f"Could not find match for {path}")
    return None


def get_schema(spec_path, operation, **kwargs):
    spec_path_data = openapi_spec['paths'][spec_path]
    if operation == 'write':
        if 'put' in spec_path_data:
            verb = 'put'   
            schema = spec_path_data[verb]["requestBody"]["content"]["application/json"]["schema"]
            responses = [int(item) for item in spec_path_data[verb]["responses"].keys()]
        elif 'post' in spec_path_data:
            verb = 'post'
            if "requestBody" in spec_path_data[verb]:
                schema = spec_path_data[verb]["requestBody"]["content"]["application/json"]["schema"]
            else:
                schema = {}
            responses = [int(item) for item in spec_path_data[verb]["responses"].keys()]
        else:
            event_handler("warning", f"{spec_path}, Error: Readonly path")
            verb = None
            schema = {}
            responses = {}
    elif operation == 'read':
        if 'get' in spec_path_data:
            verb = 'get'   
            schema = {}
            responses = [int(item) for item in spec_path_data[verb]["responses"].keys()]
        else:
            event_handler("debug", f"{spec_path} is a write-only path")
            verb = None
            schema = {}
            responses = {}
    else:
        event_handler("critical", f"Unknown schema operation {operation}.")
        exit (1)

    # if "bound_to_template" in kwargs and kwargs["bound_to_template"] == True:
    #     #
    #     # If this is for a template, we override the template schema, but keep the verb
    #     if spec_path in template_schemas:
    #         schema = template_schemas[spec_path]
    #     else:
    #         schema = {}

    return (verb, schema, responses)

def meraki_request(url, api_key, verb="get", responses=[200], payload={}, parameters=[], **kwargs):
    headers = {
        'Authorization': f'Bearer {api_key}'
    }      
    while True:
        try:
            if verb == "get":
                parameter_string = ""
                for parameter in parameters:
                    if parameter_string == "":
                        parameter_string = '?' + parameter
                    else:
                        parameter_string = parameter_string + '&' + parameter           
                response = requests.get(url + parameter_string,
                        headers =   headers,
                        timeout =   (API_CONNECT_TIMEOUT, API_TRANSMIT_TIMEOUT)
                    )                
            elif verb == "put":
                response = requests.put(url,
                        headers =   headers,
                        json    =   payload,
                        timeout =   (API_CONNECT_TIMEOUT, API_TRANSMIT_TIMEOUT)
                    )
            else:
                response = requests.post(url,
                        headers =   headers,
                        json    =   payload,
                        timeout =   (API_CONNECT_TIMEOUT, API_TRANSMIT_TIMEOUT)
                    )
            
            # Check the status code
            if response.status_code in responses:
                if response.status_code == 204:
                    return {}
                else:
                    return (response.json())
            # elif response.status_code == 401:
            #     event_handler("critical", f"{url}, Error 401: Unauthorized access - check your API key.")
            #     exit (1)
            # elif response.status_code == 404:
            #     event_handler("critical", f"{url}, Error 404: The requested resource was not found.")
            #     exit (1)
            elif response.status_code == 429:
                retry_after = response.headers.get("Retry-After")
                if retry_after is not None:
                    wait_retry = int(retry_after)
                else:
                    wait_retry = API_RETRY_DEFAULT_WAIT             
                event_handler("warning", f"Error 429: Rate limit exceeded. Retrying in {wait_retry} seconds...")
                time.sleep(wait_retry)
            else:
                try:
                    # Attempt to parse the string as JSON
                    error_resonse = json.loads(response.text)
                    if "errors" in error_resonse:
                        error_message = error_resonse["errors"][0]
                    else:
                        error_message = response.text
                except json.JSONDecodeError:
                    # If parsing fails, return the original string
                    error_message = response.text
                if expected_error_message(error_message):
                        severity = "warning"
                else:
                    severity = "error"
                event_handler(severity, f"{url}, Error {response.status_code} ({response.reason}): {error_message}")
                return None
        
        except requests.exceptions.RequestException as e:
            event_handler("critical", f"An error occurred while making the request: {e}")
            exit (1)    

def meraki_read_path(path, api_key, base_url, **kwargs) -> dict | None:
    # Get the spec path that corresponds to this path
    spec_path = get_spec_path(path)
    if spec_path == None:
        event_handler("error", f"Could not find {path} in spec.")
        return (None)
    
    if 'schema' in kwargs:
        schema = kwargs['schema']
        verb = 'get'
    else:
        verb, schema, responses = get_schema(spec_path, "read", **kwargs)
    url = f"{base_url}" + path.format(**kwargs).removesuffix('/')
    if verb == None:
        event_handler("debug", f"{path} is write-only")
        return None
    return meraki_request(url, api_key, responses=responses, **kwargs)

def meraki_write_path(path, api_key, base_url, raw_payload, api_handler=False, **kwargs) -> dict | None:
    change_needed = True
    path = path.removesuffix('/')
    # Get the spec path that corresponds to this path
    spec_path = get_spec_path(path)
    if spec_path == None:
        event_handler("error", f"Could not find {path} in spec.")
        return (None)

    if 'schema' in kwargs:
        schema = kwargs['schema']
        verb = kwargs['verb']
    else:
        verb, schema, responses = get_schema(spec_path, "write", **kwargs)

    if verb == None:
        event_handler("error", f"Unable to write to {path}")
        return {}

    full_path = path.format(**kwargs)
    url = f"{base_url}" + full_path

    # if "bound_to_template" in kwargs and kwargs["bound_to_template"] == True and schema == {}:
    #     event_handler("info", f"Network {kwargs['networkId']} is bound to a template. Ignoring {full_path}")
    #     return {}

    # See if we need a special handler for this path
    if spec_path in api_data_handlers:
        processed_payload = api_data_handlers[spec_path](path, api_key, base_url, raw_payload, **kwargs)
    else:
        processed_payload = raw_payload

    if processed_payload == None:
        return {}

    # Reduce the payload down to what is in the schema for the put/post operation 
    filtered_payload = filter_data_by_schema(processed_payload, schema)

    if hasattr(args, 'diff') and args.diff == True and not api_handler:
        show_diff = True
    else:
        show_diff = False

    #
    # Get the current data
    #
    current_data = meraki_read_path(path, args.api_key, base_url, **kwargs)
    if (current_data != None):
        write_only_url = False
        filtered_current_data = filter_data_by_schema(current_data, schema)
        #
        # Diff the current state and the proposed state
        #
        diff_dict = list(diff(filtered_current_data, filtered_payload, ignore=DIFF_IGNORE_KEYS))
    else:
        write_only_url = True
        diff_dict = []
        event_handler("debug", f"Unable to get path {full_path}")

    if filtered_payload and (diff_dict or write_only_url):
        change_needed = True
        if show_diff:
            print(color_message(path.format(**kwargs), "yellow"))
            if not write_only_url:
                # print ("Current:")
                # pprint.pp(filtered_current_data)
                # print ("New:")
                # pprint.pp(filtered_payload)
                # print ("Diff:")
                pprint.pp(diff_dict)
    else:
        change_needed = False
        if show_diff:
            print(color_message(path.format(**kwargs), "green"))

    if args.dry_run or (filtered_payload == {} and schema != {}):
        change_needed = False

    # Override the diff and always write the data
    if hasattr(args, 'always_write') and args.always_write == True:
        change_needed = True

    if change_needed:
        if args.log_level == "DEBUG":
            print(f"Writing to {url}:")
            pprint.pp(filtered_payload)
        return meraki_request(url, api_key, payload=filtered_payload, verb=verb, responses=responses, **kwargs)
    else:
        return {}

async def merakiBulkGet(session, path, api_key, base_url, semaphore):
    """
    Fetches the content of the URL using a GET request with headers.

    Parameters:
    - session (aiohttp.ClientSession): The aiohttp session.
    - url (str): The URL to fetch.
    - headers (dict): The headers to include in the request.
    - path (str): The API path to structure the results.
    - semaphore (asyncio.Semaphore): The semaphore to limit concurrent requests.

    Returns:
    - tuple: A tuple containing the path and its response.
    """
    headers = {
        'Authorization': f'Bearer {api_key}'
    }  
    url = f"{base_url}" + path
    retry = 0
    async with semaphore:
        while True:
            try:
                async with session.get(url, headers=headers) as response:
                    if response.status == 200:
                        if response.headers['Content-Type'] == 'application/json':
                            return (path, await response.json())
                        else:
                            return (path, await response.json(encoding="utf-8"))
                    elif response.status == 429:
                        retry_after = response.headers.get("Retry-After")
                     
                        if retry_after is not None:
                            wait_retry = int(retry_after)
                        else:
                            wait_retry = API_RETRY_DEFAULT_WAIT
                        event_handler("debug", f"Received {response.status} status code. Retrying url {url} in {wait_retry} seconds...")
                    # elif response.status in [400, 404]:
                    #     event_handler("error", f"{url}, Error {response.status} ({response.reason})")
                    #     return (path, {})  
                    else:
                        severity = "error"
                        error_response = await response.json()
                        if isinstance(error_response, dict) and "errors" in error_response:
                            error_message = error_response["errors"][0]
                            if expected_error_message(error_message):
                                severity = "warning"
                        else:
                            error_message = error_response
                            event_handler(severity, f"{url}, Error {response.status} ({response.reason}): {error_message}")
                        return (path, {})
                retry = retry + wait_retry
                await asyncio.sleep(retry)
            except aiohttp.ClientError as e:
                event_handler("error", f"An error occurred: {str(e)}")
            except asyncio.TimeoutError:
                event_handler("error", "The request timed out.")
            except Exception as e:
                event_handler("error", f"An unexpected error occurred: {str(e)}")

def add_to_results(result_dict, path, result):
    match = re.match('^/([^/]+)/([^/]+)(/[^{}]*)?$', path)
    if result:
        if match.group(1) not in result_dict:
            result_dict[match.group(1)] = {}
        if match.group(2) not in result_dict[match.group(1)]:
            result_dict[match.group(1)][match.group(2)] = {}
            result_dict[match.group(1)][match.group(2)]['paths'] = {}
        if match.group(3):
            path = match.group(3)
        else:
            path = "/" 
        result_dict[match.group(1)][match.group(2)]['paths'][path] = result

async def fetch_all_paths(paths, headers, base_url, max_concurrent_requests, result_dict):
    """
    Fetches the content of all URLs asynchronously with headers, limiting concurrent requests.

    Parameters:
    - urls (list): A list of tuples containing URLs and paths to fetch.
    - headers (dict): The headers to include in the request.
    - max_concurrent_requests (int): The maximum number of concurrent requests allowed.

    Returns:
    - dict: A dictionary containing all paths and their responses.
    """
    semaphore = asyncio.Semaphore(max_concurrent_requests)
    async with aiohttp.ClientSession() as session:
        tasks = [merakiBulkGet(session, path, headers, base_url, semaphore) for path in paths]
        for task in asyncio.as_completed(tasks):
            # as_completed returns an iterator, so we just have to await the iterator and not call it
            (path, result) = await task
            add_to_results(result_dict, path, result)
    return result_dict

def json_schema_type_to_python_type(schema_type: str):
    """
    Map a JSON Schema type (string) to a Python type or tuple of types,
    suitable for isinstance checks.
    """
    type_map = {
        "string": str,
        "number": (int, float),  # JSON 'number' can be int or float
        "integer": int,
        "boolean": bool,
        "array": list,
        "object": dict,
        "null": type(None),      # for checking None
    }
    return type_map.get(schema_type, None)


def filter_data_by_schema(data, schema, remove_empty_data=True):
    """
    Filters the given JSON-like data according to the given JSON Schema rules:
      1) Only keep keys present in the schema's "properties".
      2) If the data type for a key doesn't match the schema's "type":
         - Attempt to convert it to the correct type using translate_type.
         - If conversion fails, replace it with the schema's "default" if available, otherwise remove it.
      3) If remove_empty_data=True, remove keys whose value is "empty"
         (None, empty string, empty list/dict).
      4) Recursively apply this to nested objects (type=object) and
         arrays (type=array).

    :param data:   Python object (usually a dict) representing JSON data
    :param schema: A dict representing a *simplified* JSON Schema
    :param remove_empty_data: True to remove "empty" values (default True)
    :return: A new, filtered Python object adhering to the schema
    """

    def is_empty_value(value):
        """Determine if a value should be considered 'empty'."""
        if value is None:
            return True
        if value == "None":
            return True
        if isinstance(value, str) and value.strip() == "":
            return True
        if isinstance(value, (list, dict)) and not value:
            return True
        return False

    def json_schema_type_to_python_type(schema_type):
        """
        Map a JSON Schema type to a Python type or tuple of types for isinstance().
        (Simple approach; real schemas can have more advanced type definitions.)
        """
        type_map = {
            "string": str,
            # 'number' can be int or float in JSON
            "number": (int, float),
            "integer": int,
            "boolean": bool,
            "object": dict,
            "array": list,
            "null": type(None),
        }
        return type_map.get(schema_type, object)  # fallback to `object`

    def translate_type(value, target_type):
        """
        Convert a value to the specified target type.

        Args:
            value: The value to convert.
            target_type: The type to convert to. Can be a Python type (e.g., int, float, str, bool, list, dict)
                         or a string ('int', 'float', 'str', 'bool', 'list', 'dict').

        Returns:
            The converted value.

        Raises:
            ValueError: If the conversion is not possible.
            TypeError: If the target_type is not recognized.
        """
        # Allow passing type as a string
        if isinstance(target_type, str):
            type_map = {
                'int': int,
                'float': float,
                'str': str,
                'bool': bool,
                'list': list,
                'dict': dict,
            }
            if target_type not in type_map:
                raise TypeError(f"Unknown target type string: {target_type}")
            target_type = type_map[target_type]

        # Special handling for bool (Python's bool('False') is True!)
        if target_type is bool:
            if isinstance(value, str):
                if value.lower() in ('true', '1', 'yes', 'on'):
                    return True
                elif value.lower() in ('false', '0', 'no', 'off'):
                    return False
                else:
                    raise ValueError(f"Cannot convert string '{value}' to bool")
            return bool(value)

        # For other types, just call the constructor
        try:
            return target_type(value)
        except Exception as e:
            raise ValueError(f"Cannot convert {value!r} to {target_type}: {e}")

    def filter_value_by_schema(value, subschema):
        """
        Recursively filter a single value according to a given (sub)schema.
        Returns the filtered value, or None if the value is removed.
        """
        # 1) Determine the declared type in the schema
        declared_type = subschema.get("type")
        default = subschema.get("default", None)

        # 2) If no "type" in schema, we do nothing special. 
        #    But let's enforce a fallback to avoid surprises:
        if not declared_type:
            # If there's no defined type, either keep the value as-is or return None if it's "empty"
            if remove_empty_data and is_empty_value(value):
                return None
            return value

        # 3) If declared type is "object"
        if declared_type == "object":
            # Value must be a dict
            if not isinstance(value, dict):
                # Type mismatch
                return default if default is not None else None

            # Recursively filter each property
            properties = subschema.get("properties", {})
            filtered = {}
            for key, prop_schema in properties.items():
                sub_val = value.get(key, None)
                filtered_sub_val = filter_value_by_schema(sub_val, prop_schema)
                # Skip if result is None
                if filtered_sub_val is not None:
                    # Optionally remove if empty
                    if remove_empty_data and is_empty_value(filtered_sub_val):
                        continue
                    filtered[key] = filtered_sub_val
            # Return the filtered object
            # If it's empty and remove_empty_data is True, return None
            if remove_empty_data and not filtered:
                return None
            return filtered

        # 4) If declared type is "array"
        if declared_type == "array":
            # Value must be a list
            if not isinstance(value, list):
                # Type mismatch
                return default if default is not None else None

            items_schema = subschema.get("items", {})
            filtered_list = []
            for elem in value:
                filtered_elem = filter_value_by_schema(elem, items_schema)
                if filtered_elem is not None:
                    # Optionally remove if empty
                    if remove_empty_data and is_empty_value(filtered_elem):
                        continue
                    filtered_list.append(filtered_elem)
            # Return the filtered list
            if remove_empty_data and not filtered_list:
                return None
            return filtered_list

        # 5) If declared type is one of the basic types: string, number, integer, boolean, null
        python_type = json_schema_type_to_python_type(declared_type)
        if not isinstance(value, python_type):
            # Try to convert the value to the correct type
            try:
                converted_value = translate_type(value, python_type)
                if remove_empty_data and is_empty_value(converted_value):
                    return None
                return converted_value
            except Exception:
                # Type mismatch => use default if provided
                if value is not None:
                    translate_type(value, declared_type)
                    event_handler("warning", f"Type mismatch: {value} is not of type {declared_type}")
                return default if default is not None else None

        # 6) We have a valid type. Optionally remove if empty.
        if remove_empty_data and is_empty_value(value):
            return None

        return value

    # Start the recursion at the top level
    return filter_value_by_schema(data, schema)

def print_tabular_data(data, columns_to_display, sort_by=None, ascending=True):
    """
    Formats and prints JSON-like data in a tabular format with left-justified headers and data.
    
    Args:
        data (list of dict): The input data to be formatted and printed.
        columns_to_display (list of str): The columns to be printed.
    """
    # Convert the list of dictionaries to a pandas DataFrame
    df = pd.DataFrame(data)

    # Sort the DataFrame by the specified column if provided
    if sort_by:
        df = df.sort_values(by=sort_by, ascending=ascending)

    # Calculate the max width for each column to apply uniform left-justification
    max_col_widths = {col: max(df[col].astype(str).map(len).max(), len(col)) for col in columns_to_display}

    # Left-justify column titles and data with custom formatting
    formatted_rows = []

    # Format the header row
    header_row = '  '.join([col.ljust(max_col_widths[col]) for col in columns_to_display])
    formatted_rows.append(header_row)

    # Format the data rows
    for _, row in df[columns_to_display].iterrows():
        formatted_row = '  '.join([str(row[col]).ljust(max_col_widths[col]) for col in columns_to_display])
        formatted_rows.append(formatted_row)

    # Print the formatted table
    print('\n'.join(formatted_rows))

def print_csv_data(data, columns, sort_by=None, ascending=True):
    # Convert the list of dicts to a DataFrame
    df = pd.DataFrame(data, columns=columns)

    # Alternatively, you can save the CSV to a file
    if args.output_file:
        output_file = args.output_file
        df.to_csv(output_file, index=False)
    else:
        print(df.to_csv(index=False))

#
# This returns everything after the first placeholder.  If there is nothing after the first placeholder, it returns None
#           
def get_subpath(path):
    # Use regex to find all occurrences of strings inside {}
    placeholders = list(re.finditer(r'\{(.*?)\}', path))
    
    if not placeholders:
        # If no placeholders exist, return an empty string
        return ""
    
    # Get the end position of the first placeholder
    first_placeholder_end = placeholders[0].end()
    
    # Get the remainder of the string starting with the slash after the first placeholder
    remainder = path[first_placeholder_end:]
    
    # Ensure the remainder starts with a slash or is just a slash
    if not remainder.startswith('/'):
        remainder = '/' + remainder
    
    return remainder

def get_basepath(path: str, n: int = 1, include_placeholder: bool = False):
    """
    Given a path like "/networks/{networkId}/wireless/ssids/{number}",
    and an integer n, returns a tuple:
      (placeholder_var, partial_path_up_to_nth_placeholder)

    - placeholder_var is the raw variable name, e.g. "networkId".
    - partial_path_up_to_nth_placeholder is an absolute path
      that includes or excludes the nth placeholder based on
      'include_placeholder'.

    If n exceeds the number of placeholders in the string, raises ValueError.
    """

    # 1) Split on '/' to get path segments (ignore empty segments from leading slash)
    segments = [seg for seg in path.split('/') if seg]

    # 2) Identify which segments are placeholders, collecting their indices
    #    A 'placeholder' is anything that starts with '{' and ends with '}'.
    placeholder_indices = [
        i for i, seg in enumerate(segments)
        if seg.startswith('{') and seg.endswith('}')
    ]

    # 3) Check if n is valid
    if n < 1 or n > len(placeholder_indices):
        raise ValueError(
            f"Requested placeholder number {n}, "
            f"but only {len(placeholder_indices)} placeholder(s) found."
        )

    # 4) Get the index of the nth placeholder in the path segments
    nth_placeholder_index = placeholder_indices[n - 1]

    # 5) Extract the placeholder variable name (e.g., "networkId" for "{networkId}")
    placeholder_var = segments[nth_placeholder_index].strip('{}')

    # 6) Build the partial path (up to the nth placeholder, inclusive or exclusive)
    if include_placeholder:
        # Include the placeholder in the partial path
        # => up to index 'nth_placeholder_index' (inclusive)
        end_slice = nth_placeholder_index + 1
    else:
        # Exclude the placeholder and the slash before it
        # => up to index 'nth_placeholder_index' (exclusive)
        end_slice = nth_placeholder_index

    partial_segments = segments[:end_slice]

    # 7) Join them back with slashes, prepending a leading slash for an absolute path.
    if partial_segments:
        partial_path = '/' + '/'.join(partial_segments)
    else:
        # If empty, it means the placeholder was the very first segment
        partial_path = '/'

    return partial_path, placeholder_var

#
# This returns everything up to and including the first placeholder.
#  
# def get_basepath(path):
#     # Use regex to find the first placeholder
#     match = re.search(r'\{(.*?)\}', path)
    
#     if not match:
#         # If no placeholder, return the entire string and None
#         return [path, None]
    
#     # Get the position of the first placeholder
#     first_placeholder_start = match.start()
    
#     # Get the substring before the first placeholder (excluding the trailing slash)
#     before_placeholder = path[:first_placeholder_start].rstrip('/')
    
#     # Extract the first placeholder's content
#     first_placeholder = match.group(1)
    
#     return [before_placeholder, first_placeholder]

async def export_command(args):
    #
    # Build the list of paths that we want to export
    #
    api_paths = []
    api_paths_needing_expansion = []
    categorized_paths = {}
    network_ids = []
    serial_numbers = []
    stacks_by_network = {}
    parameters = []
    serial_parameters = []
    result_dict = {}
    export_all = True

    # We are going to use a combiation of meraki python library and Rest to get the data
    # because depending on this privides the best value.
    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, inherit_logging_config=True, output_log=False)

    # The first step is to get the list of network IDs that we are going to export
    #
    # A list of network IDs was provided
    #
    if args.network_ids:
        export_all = False
        network_ids = args.network_ids
        for network_id in network_ids:
            parameters.append(f"networkIds[]={network_id}")
        networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id, parameters=parameters)
        serial_parameters = parameters
    #
    # A list of tags was provided
    #
    elif args.tags:
        export_all = False
        # parameters = ['tagsFilterType=withAnyTags']
        # for tag in args.tags:
        #     parameters.append(f"tags[]={tag}")
        # networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id, parameters=parameters)
        networks = dashboard.organizations.getOrganizationNetworks(args.org_id, tags=args.tags, total_pages='all')
        network_ids = [item["id"] for item in networks]
        for network in networks:
            serial_parameters.append(f"networkIds[]={network['id']}")
    #
    # A list of network names was provided
    #
    elif args.networks:
        export_all = False
        networks = []
        # Create a dict of networks by name
        network_list = dashboard.organizations.getOrganizationNetworks(args.org_id, total_pages='all')
        # network_list = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
        networks_by_name = {network['name']: network for network in network_list}
        # Find matches and retrieve their networkId
        for network_name in args.networks:
            for key in networks_by_name.keys():
                if network_name in key:
                    networks.append(networks_by_name[key])
                    network_id = networks_by_name[key].get('id')
                    serial_parameters.append(f"networkIds[]={network_id}")
                    network_ids.append(network_id)
    else:
        event_handler("info", f"Exporting all networks for organization {args.org_id}")
        # networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
        networks = dashboard.organizations.getOrganizationNetworks(args.org_id, tags=args.tags, total_pages='all')
        # List comprehension to extract serial numbers
        network_ids = [item["id"] for item in networks]
        #
        # Get all of the templates for export
        #
        # config_template_templates = meraki_read_path("/organizations/{organizationId}/configTemplates", args.api_key, args.base_url, organizationId=args.org_id)
        config_template_templates = dashboard.organizations.getOrganizationConfigTemplates(args.org_id)
        # List comprehension to extract serial numbers
        network_ids = network_ids + [item["id"] for item in config_template_templates]

    if export_all:
        event_handler("info", f"Exporting all networks/templates")
        devices = dashboard.organizations.getOrganizationDevices(args.org_id, total_pages='all')
    else:
        template_ids = []
        networks_by_id = {network['id']: network for network in networks}
        if not args.skip_templates:
            #
            # Get the templates that the networks are bound to
            #
            for network_id in network_ids:
                network = networks_by_id[network_id]
                #
                # Collect the config templates that the networks are bound to
                #                
                if "isBoundToConfigTemplate" in network and network["isBoundToConfigTemplate"] == True:
                    if network["configTemplateId"] not in template_ids:
                        template_ids.append(network["configTemplateId"])
            network_ids.extend(template_ids)

        event_handler("info", f"Exporting networks/templates {network_ids}")
        devices = dashboard.organizations.getOrganizationDevices(args.org_id, networkIds = network_ids, total_pages='all')
    
    devices_by_serial = {device['serial']: device for device in devices}
    # List comprehension to extract serial numbers
    serial_numbers = [item["serial"] for item in devices if item["networkId"] in network_ids]

    for network in networks:
        if "switch" in network["productTypes"]:
            #
            # Get all of the switch stacks for export
            #
            switch_stacks = meraki_read_path("/networks/{networkId}/switch/stacks", args.api_key, args.base_url, networkId=network["id"])
            if switch_stacks:
                stacks_by_network[network["id"]] = {}
                for stack in switch_stacks:
                    switch_stacks_interfaces = meraki_read_path("/networks/{networkId}/switch/stacks/{switchStackId}/routing/interfaces", args.api_key, args.base_url, networkId=network["id"], switchStackId=stack["id"])
                    stack_interface_ids = []
                    for interface in switch_stacks_interfaces:
                        stack_interface_ids.append(interface["interfaceId"])
                    stacks_by_network[network["id"]][stack["id"]] = stack_interface_ids
    #
    # Find all of the paths out of the spec that apply to what we are trying to export and categorize by context
    #
    for path, verbs in openapi_spec['paths'].items():
        # Only get paths that we can write something back to.
        path_exported = False
        if path in config["api_path_exlude"]:
            event_handler("debug", f"{path} is excuded")
            continue
        elif 'get' not in verbs.keys():
            event_handler("debug", f"{path} is write-only")
            continue
        elif not bool(set(['put', 'post']) & set(verbs.keys())):
            event_handler("debug", f"{path} is read-only")
            continue
        # elif bool(set(SKIP_TAGS) & set(verbs['get']['tags'])):
        #     event_handler("debug", f"{path} is being skipped")
        #     continue       
        else:
            context = path.split('/')[1]
            if context not in categorized_paths:
                categorized_paths[context] = {}
            categorized_paths[context][path] = verbs

    #
    # The first step is to itterate each path over the networks and devices specifed by the user.  Fully
    # expanded paths are then sent to retrieve their values.  Paths that have placeholders are stored for
    # later expansion.
    #
    for context in categorized_paths:
        paths = categorized_paths[context]
        if context == "networks":
            for network_id in network_ids:
                for path, verbs in paths.items():
                    subpath = get_subpath(path)
                    placeholders = re.findall(r'\{(.*?)\}', subpath)
                    if subpath == "/":
                        new_path = f"/networks/{network_id}"
                    else:
                        new_path = f"/networks/{network_id}{subpath}"
                    if placeholders:
                        api_paths_needing_expansion.append(new_path)
                    else:
                        api_paths.append(new_path)
        elif context == "devices":
            for serial in serial_numbers:
                for path, verbs in paths.items():
                    subpath = get_subpath(path)
                    # get_tags = verbs['get']['tags']
                    # device_type = get_product_type(devices_by_serial[serial]["model"])
                    # if subpath == "/" or device_type in get_tags:
                    placeholders = re.findall(r'\{(.*?)\}', subpath)
                    if subpath == "/":
                        new_path = f"/devices/{serial}"
                    else:
                        new_path = f"/devices/{serial}{subpath}"
                    if placeholders:
                        api_paths_needing_expansion.append(new_path)
                    else:
                        api_paths.append(new_path)
                    # else:
                    #     event_handler("debug", f"Not exporting path: {path}, subpath: {subpath} for device {serial}({device_type}) because it is not a {get_tags}")                    
        elif context == "organizations":
            for path, verbs in paths.items():
                subpath = get_subpath(path)
                placeholders = re.findall(r'\{(.*?)\}', subpath)
                if subpath == "/":
                    new_path = f"/organizations/{args.org_id}"
                else:
                    new_path = f"/organizations/{args.org_id}{subpath}"
                if placeholders:
                    api_paths_needing_expansion.append(new_path)
                else:
                    api_paths.append(new_path)
        else:
            event_handler("warning", f"Skipping {context} because it is not supported for export")
                
    # 
    # Make the initial bulk request to the async function
    #
    await fetch_all_paths(api_paths, args.api_key, args.base_url, args.max_concurrent_requests, result_dict)
    #
    # Loop expanding the apis and doing a bulk fetch until everything is expanded or we hit max tries
    #
    loop_count = 0
    while api_paths_needing_expansion and loop_count < 5:
        event_handler("debug", f"De-referenceing pass {loop_count}")
        api_paths, api_paths_needing_expansion = expand_api_paths(api_paths_needing_expansion, result_dict)
        await fetch_all_paths(api_paths, args.api_key, args.base_url, args.max_concurrent_requests, result_dict)
        loop_count = loop_count + 1

    #   continue
    #
    # Save the results to a file
    #
    if args.output_file:
        output_file = args.output_file
    else:
        if args.networks and len(args.networks) == 1:
            output_file = f"{args.org_id}_{args.networks[0]}.json"
        else:
            output_file = f"{args.org_id}.json"

    with open(output_file, 'w') as file:
        result_dict['errors'] = error_log
        result_dict['api_paths_needing_expansion'] = api_paths_needing_expansion
        json.dump(result_dict, file, indent=4)
    event_handler("info", f"Results saved to {output_file}")

def expand_api_paths(api_paths_to_expand, result_dict):
    api_paths_still_needing_expansion = []
    api_paths = []
    for path in api_paths_to_expand:
        source_key = None
        source_list = []
        basepath, key = get_basepath(path)
        if source_key := key_lookup(path):
            source_key_list = [key]
        else:
            source_key_list = [key] + ['id', 'number']
        _, context, context_item, *the_rest = basepath.split('/')
        sub_basepath = '/' + '/'.join(the_rest)
        if context in result_dict:
            if context_item in result_dict[context]:
                if sub_basepath in result_dict[context][context_item]['paths']:
                    # The path is already in the data collected
                    source_list = result_dict[context][context_item]['paths'][sub_basepath]
                else:
                    # See if we can grab the path
                    source_list = meraki_read_path(basepath, args.api_key, args.base_url)
                    if source_list:
                        add_to_results(result_dict, basepath, source_list)

                if isinstance(source_list, list) and len(source_list) > 0:
                    for try_key in source_key_list:
                        if try_key in source_list[0]:
                            source_key = try_key
                    if source_key:
                        for item in source_list:
                            if source_key in item:
                                new_path = re.sub(r'\{.*?\}', str(item[source_key]), path, count=1)
                                placeholders = re.findall(r'\{(.*?)\}', path)
                                if len(placeholders) > 1:
                                    api_paths_still_needing_expansion.append(new_path)
                                else:
                                    api_paths.append(new_path)
                            else:
                                event_handler("error", f"Key {source_key} not found in {item}")
                    else:
                        event_handler("warning", f"Unable to expand {path} because the key is not found in the source data")
                else:
                    event_handler("debug", f"Unable to expand {path} because the base path is not found in the source data")
    return api_paths, api_paths_still_needing_expansion

def import_file(filename):
        event_handler("info", f"Reading {filename}...")
        try:
            with open(filename, 'r') as file:
                if filename.endswith('json'):
                    return json.load(file)
                else:
                    return yaml.safe_load(file)
        except FileNotFoundError:
            event_handler("critical", f"Error: The file '{args.input_file}' was not found.")
            exit (1)
        except json.JSONDecodeError:
            event_handler("critical", f"Error: The file '{args.input_file}' contains invalid JSON.")
            exit (1)
        except PermissionError:
            event_handler("critical", f"Error: You do not have permission to read the file '{args.input_file}'.")
            exit (1)
        except Exception as e:
            event_handler("critical", f"An unexpected error occurred: {str(e)}")
            exit (1)

def export_file(filename, contents):
    event_handler("info", f"Writing {filename}...")
    with open(filename, 'w') as file:
        json.dump(contents, file, indent=4)

def show_command(args):
    parameters = []

    # For the show commands, we are going to use the meraki python library to get the data
    # because it makes it a bit easeier and we do not have to worry about the API paths
    # not being there.
    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, inherit_logging_config=True, output_log=False)

    if hasattr(args, 'tags') and args.tags:
        for tag in args.tags: 
            parameters.append(f"tags[]={tag}")

    if args.show_command == 'networks':
        # If a data file is provided, use that data instead of making an API call
        if args.input_file:
            import_data = import_file(args.input_file)
            networks = []
            for network_id, network_data in import_data["networks"].items():
                if "/" in network_data["paths"]:
                    networks.append(network_data["paths"]["/"])
        else:
            networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
        if args.json:
            pprint.pp(networks)
        else:
            print_tabular_data(networks, ['name', 'id', 'productTypes', 'timeZone', 'tags'], sort_by='name')      
    elif args.show_command == 'organizations':
        organizations = dashboard.organizations.getOrganizations()
        if args.json:
            pprint.pp(organizations)
        else:
            print_tabular_data(organizations, ['id', 'name'])       
    elif args.show_command == 'devices':
        # If a data file is provided, use that data instead of making an API call
        if args.input_file:
            import_data = import_file(args.input_file)
            source_org_id = next(iter(import_data["organizations"]))
            networks = import_data["organizations"][source_org_id]["paths"]["/networks"]
            networks_by_id = {network['id']: network for network in networks}            
            devices = []
            for serial, device_data in import_data["devices"].items():
                if "/" in device_data["paths"]:
                    device = device_data["paths"]["/"]
                    if device["networkId"] in networks_by_id:
                        device["network"] = networks_by_id[device["networkId"]]["name"]
                    else:
                        device["network"] = device["networkId"]
                    devices.append(device)
            if args.json:
                pprint.pp(devices)
            elif args.csv:
                print_csv_data(devices, ['name', 'serial', 'mac', 'model', 'network', 'tags'], sort_by='network')
            else:
                print_tabular_data(devices, ['name', 'serial', 'mac', 'model', 'network', 'tags'], sort_by='network')
        else:
            kwargs = {}
            if args.serial:
                kwargs["serial"] = args.serial
            # devices = meraki_read_path("/organizations/{organizationId}/devices", args.api_key, args.base_url, organizationId=args.org_id)
            devices = dashboard.organizations.getOrganizationDevices(args.org_id, total_pages='all', **kwargs)
            # networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
            networks = dashboard.organizations.getOrganizationNetworks(args.org_id, total_pages='all')
            networks_by_id = {network['id']: network for network in networks}
            # Get the devices status
            devices_statuses = dashboard.organizations.getOrganizationDevicesStatuses(args.org_id, total_pages='all')
            devices_statuses_by_serial = {device_status['serial']: device_status for device_status in devices_statuses}
            #
            # Convert Network ID to Network Name
            #
            for device in devices:
                if device["networkId"] in networks_by_id:
                    device["network"] = networks_by_id[device["networkId"]]["name"]
                else:
                    device["network"] = device["networkId"]
                if device["serial"] in devices_statuses_by_serial:
                    device["status"] = devices_statuses_by_serial[device["serial"]]["status"]
                else:
                    device["status"] = "Unknown"

            if args.json:
                pprint.pp(devices)
            elif args.csv:
                print_csv_data(devices, ['name', 'serial', 'mac', 'model', 'status', 'network', 'tags'], sort_by='network')
            else:
                print_tabular_data(devices, ['name', 'serial', 'mac', 'model', 'status', 'network', 'tags'], sort_by='network')
    elif args.show_command == 'inventory':
        # If a data file is provided, use that data instead of making an API call
        if args.input_file:
            import_data = import_file(args.input_file)
            source_org_id = next(iter(import_data["organizations"]))
            inventory = import_data["organizations"][source_org_id]["paths"]["/inventory/devices"]
        else:
            kwargs = {}
            if args.serials:
                kwargs["serials"] = args.serials
            devices = dashboard.organizations.getOrganizationInventoryDevices(args.org_id, total_pages='all', **kwargs)
            # inventory = meraki_read_path("/organizations/{organizationId}/inventory/devices", args.api_key, args.base_url, organizationId=args.org_id)
            networks = dashboard.organizations.getOrganizationNetworks(args.org_id, total_pages='all')
            networks_by_id = {network['id']: network for network in networks}
            # Get the devices status
            devices_statuses = dashboard.organizations.getOrganizationDevicesStatuses(args.org_id, total_pages='all')
            devices_statuses_by_serial = {device_status['serial']: device_status for device_status in devices_statuses}
            #
            # Convert Network ID to Network Name
            #
            for device in devices:
                if device["networkId"] in networks_by_id:
                    device["network"] = networks_by_id[device["networkId"]]["name"]
                else:
                    device["network"] = device["networkId"]
                if device["serial"] in devices_statuses_by_serial:
                    device["status"] = devices_statuses_by_serial[device["serial"]]["status"]
                    device["lanIp"] = devices_statuses_by_serial[device["serial"]]["lanIp"]
                else:
                    device["status"] = "Inventory"
                    device["lanIp"] = ""
        if devices:
            if args.json:
                pprint.pp(devices)
            elif args.csv:
                print_csv_data(devices, ['name', 'serial', 'mac', 'lanIp', 'model', 'status', 'network', 'tags'], sort_by='network')
            else:
                print_tabular_data(devices, ['name', 'serial', 'mac', 'lanIp', 'model', 'status', 'network', 'tags'], sort_by='network')
    elif args.show_command == 'templates':
        if args.input_file:
            import_data = import_file(args.input_file)
            source_org_id = next(iter(import_data["organizations"]))
            network_ids = import_data["networks"].keys()
            org_templates = import_data["organizations"][source_org_id]["paths"]["/configTemplates"]
            templates = []
            for template in org_templates:
                if template["id"] in network_ids:
                    templates.append(template)
        else:        
            templates = meraki_read_path("/organizations/{organizationId}/configTemplates", args.api_key, args.base_url, organizationId=args.org_id)
        if args.json:
            pprint.pp(templates)
        else:
            print_tabular_data(templates, ['id', 'name', 'productTypes', 'timeZone'])
    elif args.show_command == 'admins':
        # If a data file is provided, use that data instead of making an API call
        if args.input_file:
            import_data = import_file(args.input_file)
            source_org_id = next(iter(import_data["organizations"]))            
            if "/admins" in import_data["organizations"][source_org_id]["paths"]:
                admins = import_data["organizations"][source_org_id]["paths"]["/admins"]
            else:
                admins = []
            networks = []
            # Need to get networks to dereference IDs
            for network_id, network_data in import_data["networks"].items():
                if "/" in network_data["paths"]:
                    networks.append(network_data["paths"]["/"])
        else:
            admins = meraki_read_path("/organizations/{organizationId}/admins", args.api_key, args.base_url, organizationId=args.org_id)
            # Need to get networks to dereference IDs
            networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)


        # Dereference the network IDs to network names
        networks_by_id = {network['id']: network for network in networks}
        for admin in admins:
            for item in admin['networks']:
                if item["id"] in networks_by_id:
                    item["name"] = networks_by_id[item["id"]]["name"]
                    item.pop("id")

        if args.json:
            pprint.pp(admins)
        elif args.csv:
            # Convert the list of dicts to a DataFrame
            df = pd.DataFrame(admins)

            # Print the DataFrame as CSV to the console
            # print(df.to_csv(index=False))

            # Alternatively, you can save the CSV to a file
            if args.output_file:
                output_file = args.output_file
            else:
                output_file = f"{args.org_id}_admins.csv"
            df.to_csv(output_file, index=False)

        else:
            print_tabular_data(admins, ['name', 'email', 'accountStatus', 'hasApiKey', 'lastActive', 'networks'], sort_by='name')      
    elif args.show_command == 'client-policy':
        all_clients = []
        networks = dashboard.organizations.getOrganizationNetworks(args.org_id, total_pages='all')
        for network in networks:
            policies_by_client = dashboard.networks.getNetworkPoliciesByClient(network["id"], timespan=args.timespan*86400, total_pages='all')
            # clients = dashboard.networks.getNetworkClients(network["id"], timespan=args.timespan*86400, total_pages='all')
            for client in policies_by_client:
                client["network"] = network["name"]
                client["networkId"] = network["id"]
                all_clients.append(client)
        if args.output_file:
            export_file(args.output_file, all_clients)
        else:
            pprint.pp(all_clients)
    elif args.show_command == 'me':
        organizations = dashboard.organizations.getOrganizations()
        me = dashboard.administered.getAdministeredIdentitiesMe()
        org_name = "Unknown"
        for org in organizations:
            if org["id"] == args.org_id:
                org_name = org["name"]

        print (f"Name:            {me['name']}")
        print (f"Email:           {me['email']}")
        print (f"MERAKI_ORG_ID:   {args.org_id} ({org_name})")
        print (f"MERAKI_BASE_URL: {args.base_url}")
    else:
        event_handler("critical", f"Unknown show command {args.show_command}")

def parse_path(path_string):
    """
    Given an API-like path (always starting with '/'), split it into
    three parts: context, context_item (optional), and subpath (optional).

    Examples:
      1) "/networks/{networkId}/wireless/ssids/{number}/identityPsks/{identityPskId}"
         => context="networks"
            context_item="{networkId}"
            subpath="/wireless/ssids/{number}/identityPsks/{identityPskId}"

      2) "/networks/{networkId}"
         => context="networks"
            context_item="{networkId}"
            subpath=None

      3) "/networks"
         => context="networks"
            context_item=None
            subpath=None
    """

    # Remove the leading slash
    trimmed = path_string.lstrip('/')
    # Split on slash up to 2 times, so we get at most 3 parts
    parts = trimmed.split('/', 2)

    # We want exactly three variables: context, context_item, subpath
    # If there are fewer than 3 parts, pad with None
    context, context_item, subpath = parts + [None] * (3 - len(parts))

    # If subpath exists, prepend a slash
    if subpath is not None:
        subpath = '/' + subpath

    return context, context_item, subpath

def extract_path_variables(path: str, path_spec: str) -> dict:
    """
    Given a path (e.g. '/networks/L_123/appliance/vlans/5') and
    a path_spec (e.g. '/networks/{networkId}/appliance/vlans/{vlanId}'),
    return a dictionary mapping the placeholder variables (e.g. networkId, vlanId)
    to their corresponding values in the path (e.g. L_123, 5).

    Assumptions:
      - The path and path_spec have the same number of segments.
      - Any segments in path_spec enclosed in '{}' are treated as variable names.
      - Non-placeholder segments should match exactly, or you can decide how to handle mismatches.

    Example:
      path       = "/networks/L_1155173304420542812/appliance/vlans/5"
      path_spec  = "/networks/{networkId}/appliance/vlans/{vlanId}"
      returns => {'networkId': 'L_1155173304420542812', 'vlanId': '5'}
    """

    # 1) Strip leading/trailing slashes and split into segments
    path_segments = path.strip('/').split('/')
    spec_segments = path_spec.strip('/').split('/')

    if len(path_segments) != len(spec_segments):
        raise ValueError(
            f"Segment count mismatch:\n"
            f"  path has {len(path_segments)} segments: {path_segments}\n"
            f"  spec has {len(spec_segments)} segments: {spec_segments}"
        )

    variables = {}

    # 2) Iterate over segments in the spec
    for p_seg, s_seg in zip(path_segments, spec_segments):
        # Check if it's a placeholder (e.g. "{networkId}")
        if s_seg.startswith('{') and s_seg.endswith('}'):
            var_name = s_seg[1:-1]  # remove surrounding braces
            variables[var_name] = p_seg
        else:
            # If it's not a placeholder, optionally verify exact match
            if p_seg != s_seg:
                raise ValueError(
                    f"Non-placeholder segment mismatch: spec='{s_seg}', path='{p_seg}'. "
                    "Adjust error handling as needed."
                )

    return variables

def meraki_get_path(spec_path, **kwargs):
    if spec_path in openapi_spec['paths']:
        spec_path_data = openapi_spec['paths'][spec_path]
        if "get" in spec_path_data:
            if "operationId" in spec_path_data["get"]:
                operation_id = spec_path_data["get"]["operationId"]
            else:
                event_handler("error", f"Operation ID not found for {spec_path}")
                return None
        else:
            event_handler("error", f"GET method not found for {spec_path}")
            return None
    else:
        event_handler("error", f"Cannot find {spec_path} in OpenAPI spec")
        return None
    try:
        result = dashboard.appliance.getOrganizationApplianceSecurityEvents(args.org_id, **kwargs)
        return result
    except meraki.AsyncAPIError as e:
        event_handler("error", f"Error: {e}")
        return None

def import_path(path, api_key, base_url, raw_payload, **kwargs):
    if spec_path := get_spec_path(path):
        if spec_path in api_path_handlers:
            unique_key, instance_key = api_path_handlers[spec_path]
            basepath, placeholder_var = get_basepath(spec_path, 2)
            existing_item = False
            event_handler("debug", f"Using default path handler for {path}")

            # Get the variables needed for the API call
            # path_variables = extract_path_variables(path, spec_path)
            # Get the basepath for the read
            
            basepath_data = meraki_read_path(basepath, args.api_key, args.base_url, **kwargs)
            if isinstance(basepath_data, list):
                for item in basepath_data:
                    if unique_key in item:
                        if item[unique_key] == raw_payload[unique_key]:
                            event_handler("debug", f"Found {unique_key} == {raw_payload[unique_key]} in {basepath}")
                            existing_item = True
                            if instance_key in item:
                                instance = item[instance_key]
                            else:
                                event_handler("error", f"Cannot find instance key for {basepath}")
                                return None
            else:
                event_handler("error", f"Error reading {basepath}")
                return None
            
            if existing_item:
                event_handler("debug", f"Writing to {basepath}/{instance}")
                result = meraki_write_path(f"{basepath}/{instance}" , api_key, base_url, raw_payload, **kwargs)
            else:
                event_handler("debug", f"Writing to {basepath}")
                result = meraki_write_path(basepath, api_key, base_url, raw_payload, **kwargs)
        else:
            result = meraki_write_path(path, api_key, base_url, raw_payload, **kwargs)
    else:
        event_handler("error", f"Path {path} not found in OpenAPI spec")

#
# Retreive the data from the import data
#
def get_from_import(path):
    global import_data
    if import_data == {}:
        if args.input_file:
            import_data = import_file(args.input_file)
        else:
            event_handler("error", f"Import data not found")
            return None
    _, context, context_item, *the_rest = path.split('/')
    if the_rest:
        subpath = '/' + '/'.join(the_rest)
    else:
        subpath = '/'
    # Replace with the source org ID from the import file
    if context == "organizations":
        context_item = next(iter(import_data["organizations"]))
    if context in import_data:
        if context_item in import_data[context]:
            if subpath in import_data[context][context_item]["paths"]:
                print (f"Found {subpath} in import data")
                return import_data[context][context_item]["paths"][subpath]
            else:
                return None
        else:
            event_handler("error", f"Cannot find {path} in import data")
            return None
    else:
        event_handler("error", f"Cannot find {path} in import data")
        return None
        
def translate_path(path):
    spec_path = get_spec_path(path)
    if spec_path in api_path_handlers:
        unique_key, instance_key = api_path_handlers[spec_path]
    else:
        event_handler("error", f"Path {path} not api_path_handlers")
        return None
    # Get the data from the import file
    import_path_data = get_from_import(path)
    if not import_path_data:
        event_handler("error", f"Cannot find {path} in import data")
        return None
    
    event_handler("debug", f"Translating Path {path}, {unique_key}: {import_path_data[unique_key]}")
    basepath, placeholder_var = get_basepath(spec_path, 2)
    _, context, context_item, *the_rest = basepath.split('/')
    # Make sure that we are querying the target org
    if context == "organizations":
        context_item = args.org_id
    basepath = '/' + '/'.join([context, context_item] + the_rest)
    basepath_data = meraki_read_path(basepath, args.api_key, args.base_url)
    if basepath_data:
        if isinstance(basepath_data, list):
            for item in basepath_data:
                if unique_key in item:
                    if item[unique_key] == import_path_data[unique_key]:
                        event_handler("debug", f"Found {item[unique_key]}:{instance_key} = {item[instance_key]}")
                        return item[instance_key]
        else:
            event_handler("error", f"Error reading {basepath}: Not a list")
            return None
    else:
        event_handler("error", f"Error reading {basepath}")
        return None

def import_command(args):
    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, simulate=args.dry_run, output_log=False)
    #
    # Open the file that we are trying to import
    #
    import_data = import_file(args.input_file)

    if len(import_data["organizations"]) == 0:
        event_handler("critical", "Organization data missing from import file")
        exit (1)
    elif len(import_data["organizations"]) > 1 and args.source_org_id == None:
        event_handler("critical", "Multiple Organizations in import file. `--source-org-id` must be specified.")
        exit (1)
    elif len(import_data["organizations"]) == 1 and args.source_org_id == None:
        source_org_id = next(iter(import_data["organizations"]))
    else:
        source_org_id = args.source_org_id

    if args.import_command == "templates":
        source_config_templates = import_data["organizations"][source_org_id]["paths"]["/configTemplates"]
        target_config_templates = meraki_read_path("/organizations/{organizationId}/configTemplates", args.api_key, args.base_url, organizationId=args.org_id)
        # target_config_template_names = [item["name"] for item in target_config_templates]
        #
        # Create the Templates if the do not already exist
        #
        for source_config_template in source_config_templates:
            if source_config_template["id"] in import_data["networks"]:                
                target_config_template_id = None
                for target_config_template in target_config_templates:
                    if target_config_template["name"] == source_config_template["name"]:
                        event_handler("debug", "Found existing template {source_config_template['name']} ({target_config_template['id']}).")
                        target_config_template_id = target_config_template["id"]
                #
                # If no template was found by that name, it needs to be created
                #
                if target_config_template_id == None:
                    event_handler("info", f"Creating config template {source_config_template['name']}")
                    result = import_path("/organizations/{organizationId}/configTemplates", args.api_key, args.base_url, source_config_template, organizationId=args.org_id)
                    if args.dry_run:
                        pass
                    elif result and "id" in result:
                        target_config_template_id = result["id"]
                    else:
                        event_handler("critical", f"Error creating template {source_config_template['name']}")
                        exit (1)
                #
                # Load/Update the template data
                #
                source_config_template_data = import_data["networks"][source_config_template["id"]]["paths"]
                for sub_path, config_template_data in source_config_template_data.items():
                    if args.subpath and not sub_path.startswith(args.subpath):
                        # event_handler("debug", f"Skipping import of {sub_path}")
                        continue
                    else:
                        path = "/networks/{networkId}" + sub_path
                        import_path(path, args.api_key, args.base_url, config_template_data, networkId=target_config_template_id, is_template=True)
            else:
                event_handler("error", f"Skipping import of template {source_config_template['name']}({source_config_template['id']}) because it is not found in import data.")
    elif args.import_command == "networks":     
        target_networks = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
        if args.source_network_ids:
            source_network_id_list = args.source_network_ids
        else:
            source_network_id_list = import_data["networks"].keys()
        for source_network_id in source_network_id_list:
            if source_network_id not in import_data["networks"]:
                event_handler("error", f"Skipping import of network {source_network_id} because it is not found in import data.")
                continue
            #
            # Map the source network to the desintation network by name
            #
            bound_to_template = False    
            
            if '/' in import_data["networks"][source_network_id]['paths']:
                source_network_data = import_data["networks"][source_network_id]['paths']['/']
            else:
                event_handler("warning", f"Root for {source_network_id} was not found in import data (Probably a template).")
                continue
            source_network_name = source_network_data['name']     
            target_network_id = None
            for target_network in target_networks:
                if target_network["name"] == source_network_name:
                    target_network_id = target_network["id"]
                    target_network_data = target_network

            if target_network_id == None:
                event_handler("info", f"Creating network {source_network_name}")
                result = meraki_write_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, source_network_data, organizationId=args.org_id)
                if result and "id" in result:
                    target_network = result
                    target_network_data = result
                    target_network_id = target_network["id"]
                else:
                    event_handler("critical", f"Error creating network {source_network_name}")
                    exit (1)

            #
            # Bind the network to the template if present
            #
            if "configTemplateId" in source_network_data:
                bound_to_template = True
                #
                # Find the name of the template associated with the source network's config template
                #
                source_network_template_id = source_network_data["configTemplateId"]
                source_network_template_name = None
                source_network_templates = import_data["organizations"][source_org_id]["paths"]["/configTemplates"]
                for source_network_template in source_network_templates:
                    if source_network_template["id"] == source_network_template_id:
                        source_network_template_name = source_network_template["name"]
                if source_network_template_name == None:
                    event_handler("critical", f"Could not find name for template ID: {source_network_template_id} in org {args.org_id}")
                    exit (1)
                #
                # If we find the name, find the template in the target network with the same name
                #
                target_config_templates = meraki_read_path("/organizations/{organizationId}/configTemplates", args.api_key, args.base_url, organizationId=args.org_id)                    
                target_config_template_id = None
                for target_config_template in target_config_templates:
                    if target_config_template["name"] == source_network_template_name:
                        target_config_template_id = target_config_template["id"]
                if target_config_template_id == None:
                    event_handler("critical", f"Could not find template {source_network_template_name} in org {args.org_id}")
                    exit (1)     

                if "configTemplateId" in target_network_data and target_network_data["configTemplateId"] != target_config_template_id:
                    event_handler("error", f"Target network bound to wrong template ({target_config_template_id} != {target_network['configTemplateId']}). We need to unbind and rebind this template")
                elif "configTemplateId" in target_network_data and target_network_data["configTemplateId"] == target_config_template_id:
                    # No change needed
                    pass
                else:
                    #
                    # Bind the network to the template
                    # 
                    event_handler("debug", f"Binding network {source_network_name} to template {source_network_template_name}")
                    response = dashboard.networks.bindNetwork(target_network_id, target_config_template_id, autoBind=False)
                    print (response)                
            if not args.create_only:                                       
                #
                # Load/Update the network data
                #
                kwargs = {}
                kwargs["networkId"] = target_network_id
                kwargs["bound_to_template"] = bound_to_template
                source_network_paths = import_data["networks"][source_network_id]["paths"]
                if "/switch/stacks" in source_network_paths and source_network_paths["/switch/stacks"]:
                    # The source network had switch stacks, so we need to map them to the ones in the new network
                    #
                    # Get all of the switch stacks in the target network for mapping
                    #
                    target_stack_map = {} # by Name
                    source_stack_map = {} # by Id
                    target_switch_stacks = meraki_read_path("/networks/{networkId}/switch/stacks", args.api_key, args.base_url, networkId=target_network_id)

                    for stack in target_switch_stacks:
                        target_stack_map[stack["name"]] = stack
                    for stack in source_network_paths["/switch/stacks"]:
                        if stack["name"] in target_stack_map:
                            source_stack_map[stack["id"]] = target_stack_map[stack["name"]]["id"]
                        else:
                            event_handler("error", f"Mapping for source stack {stack['name']} ({stack['id']}) not found")
                for sub_path, network_data in source_network_paths.items():
                    path = "/networks/{networkId}" + sub_path
                    if args.subpath and not sub_path.startswith(args.subpath):
                        # event_handler("debug", f"Skipping import of {sub_path}")
                        continue
                    else:                    
                        #
                        # Handle the endpoints for each of the SSIDs
                        #
                        # if wireless_ssid_match := re.match('^/networks/{networkId}/wireless/ssids/([0-9]+)/([^{}]+)$', path):
                        #     kwargs["number"] = wireless_ssid_match.group(1)
                        #     path = "/networks/{networkId}/wireless/ssids/{number}/" + wireless_ssid_match.group(2)
                        #
                        # Handle the Stacks
                        #
                        # stack_match = re.match('^/networks/{networkId}/switch/stacks/([^/]+)/?([^{}]*)$', path)
                        # if stack_match:
                        #     source_switchStackId = stack_match.group(1)
                        #     print (f"Stack: {source_switchStackId}")
                        #     if source_switchStackId in source_stack_map:
                        #         kwargs["switchStackId"] = source_stack_map[source_switchStackId]
                        #         event_handler("debug", f"Mapping source stack: {source_switchStackId} = {kwargs['switchStackId']}")
                        #     else:
                        #         event_handler("error", f"Mapping for source stack {source_switchStackId} not found")
                        #         continue
                        #     path = "/networks/{networkId}/switch/stacks/{switchStackId}/" + stack_match.group(2)
                        #
                        # Write the data to the path
                        #        
                        import_path(path, args.api_key, args.base_url, network_data, is_template=bound_to_template, **kwargs)
    elif args.import_command == "devices":
        for serial in import_data["devices"]:
            # if args.product_types and (sub_path.split('/')[1] in PRODUCT_TYPES and sub_path.split('/')[1] not in args.product_types):
            #     logging.debug(f"Skipping {sub_path}")
            #     continue             
            if args.serials != None and serial not in args.serials:
                event_handler("debug", f"Skipping {serial}")
                continue
            if args.source_network_ids and import_data["devices"][serial]["paths"]['/']["networkId"] not in args.source_network_ids:
                event_handler("debug", f"Skipping {serial} because it is not in source networks list")
                continue
            source_network_paths = import_data["devices"][serial]["paths"]
            for sub_path, device_data in source_network_paths.items():
                path = "/devices/{serial}" + sub_path
                #
                # Write the data to the path
                #
                # Should we remove None values everywhere?
                if "switchProfileId" in device_data and device_data["switchProfileId"] == None:
                    device_data.pop("switchProfileId", None)
                import_path(path, args.api_key, args.base_url, device_data, serial=serial)
    elif args.import_command == "organizations":
            # Get the key and the value from the dictionary with a single key
            org_import_data = import_data["organizations"]
            key = next(iter(org_import_data))
            source_org_paths = org_import_data[key]["paths"]
            for sub_path, path_data in source_org_paths.items():
                if args.subpath and not sub_path.startswith(args.subpath):
                    # event_handler("debug", f"Skipping import of {sub_path}")
                    event_handler("debug", f"Skipping {sub_path}")
                    continue
                path = "/organizations/{organizationId}" + sub_path
                #
                # Write the data to the path
                #
                import_path(path, args.api_key, args.base_url, path_data, organizationId=args.org_id)

def claim_command(args):
    claim_list = []

    if args.inventory:
        if args.serials:
            claim_list = args.serials
        elif args.input_file:
            import_data = import_file(args.input_file)
            source_org_id = next(iter(import_data["organizations"]))
            inventory = import_data["organizations"][source_org_id]["paths"]["/inventory/devices"]
            claim_list = [item["serial"] for item in inventory]
        
        if claim_list:
            print (f"Claim {claim_list} into inventory")
            answer = prompt_user(f"Proceed?", ["Yes", "No"])
            if answer == "Yes":
                claim_serials_into_inventory(claim_list)
        else:
            event_handler("info", "No serials to claim")
    else:
        import_data = {}
        if args.input_file:
            import_data = import_file(args.input_file)
        # We always get the list of networks from the API for the target network
        target_network_list = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
        # Create a dict of networks by name
        target_networks_by_name = {network['name']: network for network in target_network_list}
        target_networks_by_id = {network['id']: network for network in target_network_list}

        # First, check to see if the target network ID or name is provided.  It name, need to convert to ID
        target_network_id = None
        if args.network_id:
            target_network_id = args.network_id
        elif args.network:
            # Find matches and retrieve their networkId
            for key in target_networks_by_name.keys():
                if args.network in key:
                    if target_network_id:
                        event_handler("critical", f"Multiple networks found with name {args.network}")
                    target_network_name = target_networks_by_name[key].get('name')
                    target_network_id = target_networks_by_name[key].get('id')

        # now that we have the source and target network IDs, we need to get the list of devices to claim
        if args.serials:
            claim_list = args.serials
            if target_network_id:
                print (f"Claiming {claim_list} into network {target_network_id}")
                answer = prompt_user(f"Proceed?", ["Yes", "No"])
                if answer == "Yes":
                    claim_serials_into_network(claim_list, target_network_id)
            else:
                event_handler("critical", "Target network ID must be provided when serials are provided.")
        # Next, check to see if an export file was proved.  If so, we need to get the list of devices from the file
        elif import_data:
            # If an input file is provided, we get the list of devices from the file
            org_keys = list(import_data["organizations"].keys())
            source_network_list = import_data["organizations"][org_keys[0]]["paths"]["/networks"]
            device_by_serial = import_data["devices"]

            # If we have an export file, we need to figure out the network from which to get the devices to claim.
            # If we were given a specific source-network, then then use it.  Otherwise, use the target network name.
            source_network_id = None
            if args.source_network_id:
                source_network_id = args.source_network_id
            elif args.source_network_name:
                # Create a dict of networks by name
                source_networks_by_name = {network['name']: network for network in source_network_list}
                # Find matches and retrieve their networkId
                for key in source_networks_by_name.keys():
                    if args.source_network_name in key:
                        if source_network_id:
                            event_handler("critical", f"Multiple networks found with name {args.source_network_name}")
                        source_network_id = source_networks_by_name[key].get('id')
                        source_network_name = source_networks_by_name[key].get('name')
            elif args.network:
                # Create a dict of networks by name
                source_networks_by_name = {network['name']: network for network in source_network_list}
                # Find matches and retrieve their networkId
                for key in source_networks_by_name.keys():
                    if args.network in key:
                        if source_network_id:
                            event_handler("critical", f"Multiple networks found with name {args.network}")
                        source_network_id = source_networks_by_name[key].get('id')
                        source_network_name = source_networks_by_name[key].get('name')

            if source_network_id:
                event_handler("debug", f"Claiming from network {source_network_id}")
            else:
                claim_by_network = {}
                event_handler("debug", "Claiming all devices in import file.")
                source_networks_by_id = {network['id']: network for network in source_network_list}
                devices = import_data["devices"]
                for device in devices:
                    device_data = devices[device]["paths"]["/"]
                    if device_data["networkId"] in source_networks_by_id:
                        target_network_name = source_networks_by_id[device_data["networkId"]]["name"]
                        event_handler("debug", f"Found {device_data['serial']} in {target_network_name}")
                    else:
                        event_handler("error", "Unable to find source network for device {device_data['serial']}")
                        continue
                    if target_network_name in target_networks_by_name:
                        target_network_id = target_networks_by_name[target_network_name]["id"]
                        event_handler("debug", f"Found Target network ID {target_network_id}")
                    else:
                        event_handler("error", f"Unable to find target network ID for {target_network_name}")
                        continue
                    if target_network_id in claim_by_network:
                        claim_by_network[target_network_id].append(device_data["serial"])
                    else:
                        claim_by_network[target_network_id] = [device_data["serial"]]
                pprint.pp(claim_by_network)
                answer = prompt_user(f"Proceed?", ["Yes", "No"])
                if answer == "Yes":            
                    for target_network_id, serials in claim_by_network.items():
                        event_handler("info", f"Claiming {serials} in network {target_networks_by_id[target_network_id]['name']} ({target_network_id})")
                        claim_serials_into_network(serials, target_network_id)
        else:
            event_handler("critical", "Input file must be provided when serials is not provided.")     

def claim_serials_into_inventory(serials):
    #
    # See what devices are already claimed in the network
    #
    unclaimed_serials = []

    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, simulate=args.dry_run, output_log=False)

    # Get the current inventory:
    inventory = dashboard.organizations.getOrganizationInventoryDevices(args.org_id, total_pages='all')
    # inventory = meraki_read_path("/organizations/{organizationId}/inventory/devices", args.api_key, args.base_url, organizationId=args.org_id)
    inventory_serials = [item["serial"] for item in inventory]
    unclaimed_serials = [serial for serial in serials if serial not in inventory_serials]
    if unclaimed_serials:
        event_handler("info", f"Claiming {unclaimed_serials} into inventory")
        response = dashboard.organizations.claimIntoOrganizationInventory(
            args.org_id, 
            # orders=['4CXXXXXXX'], 
            serials=unclaimed_serials, 
            # licenses=[{'key': 'Z2XXXXXXXXXX', 'mode': 'addDevices'}]
        )            
    else:
        event_handler("info", f"No serials to claim in inventory")

def claim_serials_into_network(serials, network_id):
    #
    # See what devices are already claimed in the network
    #
    unclaimed_serials = []

    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, simulate=args.dry_run, output_log=False)

    device_data = meraki_read_path("/networks/{networkId}/devices", args.api_key, args.base_url, networkId=network_id)
    if device_data:
        existing_serials = [device["serial"] for device in device_data]
    else:
        existing_serials = []

    unclaimed_serials = [serial for serial in serials if serial not in existing_serials]
    if unclaimed_serials:
        event_handler("debug", f"Claiming {unclaimed_serials} in network {network_id}")
        response = dashboard.networks.claimNetworkDevices(
            network_id, unclaimed_serials
        )          
    else:
        event_handler("debug", f"No serials to claim in network {network_id}")

def unclaim_command(args):
    unclaim_list = []
    import_data = {}

    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, simulate=args.dry_run, output_log=False)

    # First we need to figure out where to get the data that we need, either
    # an input file or the API
    if args.input_file:
        # If an input file is provided, we get the list of devices from the file
        import_data = import_file(args.input_file)
        src_org_id = next(iter(import_data["organizations"]))
        network_list = import_data["organizations"][src_org_id]["paths"]["/networks"]
        device_by_serial = import_data["devices"]
    else:
        # Otherwise we get the list of networks from the API
        network_list = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)

    # Then, check to see if networks are provided.  If the names are provided, we need to get their networkIds
    network_id_list = []
    if args.network_ids:
        network_id_list = args.networks
    elif args.networks:
        # Create a dict of networks by name
        networks_by_name = {network['name']: network for network in network_list}
        # Find matches and retrieve their networkId
        network_id_list = []
        for network_name in args.networks:
            for key in networks_by_name.keys():
                if network_name in key:
                    network_id = networks_by_name[key].get('id')
                    network_id_list.append(network_id)

    if network_id_list:
        event_handler("debug", f"Unclaiming devices from networks {network_id_list}")
        # If we have a list of network IDs, we need to find the devices in those networks to unclaim
        if args.input_file:
            # An input file was provided, so we need to get the devices from the file
            for serial, data in device_by_serial.items():
                device_data = data["paths"]["/"]
                if device_data["networkId"] in network_id_list:
                    unclaim_list.append({"serial": serial, "networkId": device_data["networkId"]})
        else:
            # An input file was not provided, so we need to get the devices from the API
            parameters = []
            # Build the paramater list to limit the devices to the networks provided
            for network_id in network_id_list:
                parameters.append(f"networkIds[]={network_id}")
            # If we have networkIds provided by the user, find the devices in those networks
            device_list = meraki_read_path("/organizations/{organizationId}/devices", args.api_key, args.base_url, organizationId=args.org_id, parameters=parameters)
            if device_list:
                for device in device_list:
                    unclaim_list.append({"serial": device["serial"], "networkId": device["networkId"]})

    elif args.serials:
        # If we were given a list of serial numbers, we need to figure out what network they are in
        if not import_data:
            device_list = meraki_read_path("/organizations/{organizationId}/devices", args.api_key, args.base_url, organizationId=args.org_id)
        devices_by_serial = {device['serial']: device for device in device_list}
        for serial in args.serials:
            if serial in devices_by_serial:
                unclaim_list.append({"serial": serial, "networkId": devices_by_serial[serial]["networkId"]})

    elif args.input_file:
        # If the network_id_list is empty, we need to get all devices in the input file
        for serial, data in device_by_serial.items():
            device_data = data["paths"]["/"]
            unclaim_list.append({"serial": serial, "networkId": device_data["networkId"]})

    if unclaim_list:
        print (unclaim_list)
        answer = prompt_user(f"Unclaim the these devices?", ["Yes", "No"])
        if answer == "Yes":
            for item in unclaim_list:
                event_handler("info", f"Rebooting {item['serial']}.")
                payload = {}
                response = dashboard.devices.rebootDevice(item['serial'])
                event_handler("info", f"Removing {item['serial']} from network {item['networkId']}")
                response = dashboard.networks.removeNetworkDevices(item['networkId'], item['serial'])
            serials_list = [item["serial"] for item in unclaim_list]
            event_handler("debug", f"Unclaiming {serials_list} in org {args.org_id}")
            response = dashboard.organizations.releaseFromOrganizationInventory(
                args.org_id, 
                serials=serials_list
            )            
        event_handler("warning", "No devices to unclaim.")

def remove_command(args):
    remove_list = []
    import_data = {}

    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, simulate=args.dry_run, output_log=False)

    # First we need to figure out where to get the data that we need, either
    # an input file or the API
    if args.input_file:
        # If an input file is provided, we get the list of devices from the file
        import_data = import_file(args.input_file)
        src_org_id = next(iter(import_data["organizations"]))
        network_list = import_data["organizations"][src_org_id]["paths"]["/networks"]
        device_by_serial = import_data["devices"]
    else:
        # Otherwise we get the list of networks from the API
        # network_list = meraki_read_path("/organizations/{organizationId}/networks", args.api_key, args.base_url, organizationId=args.org_id)
        network_list = dashboard.organizations.getOrganizationNetworks(args.org_id, total_pages='all')

    # Then, check to see if networks are provided.  If the names are provided, we need to get their networkIds
    network_id_list = []
    if args.network_ids:
        network_id_list = args.networks
    elif args.networks:
        # Create a dict of networks by name
        networks_by_name = {network['name']: network for network in network_list}
        # Find matches and retrieve their networkId
        network_id_list = []
        for network_name in args.networks:
            for key in networks_by_name.keys():
                if network_name in key:
                    network_id = networks_by_name[key].get('id')
                    network_id_list.append(network_id)

    if network_id_list:
        event_handler("debug", f"Unclaiming devices from networks {network_id_list}")
        # If we have a list of network IDs, we need to find the devices in those networks to unclaim
        if args.input_file:
            # An input file was provided, so we need to get the devices from the file
            for serial, data in device_by_serial.items():
                device_data = data["paths"]["/"]
                if device_data["networkId"] in network_id_list:
                    remove_list.append({"serial": serial, "networkId": device_data["networkId"]})
        else:
            # An input file was not provided, so we need to get the devices from the API
            parameters = []
            # Build the paramater list to limit the devices to the networks provided
            for network_id in network_id_list:
                parameters.append(f"networkIds[]={network_id}")
            # If we have networkIds provided by the user, find the devices in those networks
            device_list = meraki_read_path("/organizations/{organizationId}/devices", args.api_key, args.base_url, organizationId=args.org_id, parameters=parameters)
            if device_list:
                for device in device_list:
                    remove_list.append({"serial": device["serial"], "networkId": device["networkId"]})

    elif args.serials:
        kwargs = {}
        kwargs["serials"] = args.serials
        # If we were given a list of serial numbers, we need to figure out what network they are in
        if not import_data:
            inventory = dashboard.organizations.getOrganizationInventoryDevices(args.org_id, total_pages='all', **kwargs)
            # device_list = meraki_read_path("/organizations/{organizationId}/devices", args.api_key, args.base_url, organizationId=args.org_id)
            remove_list = inventory
        else:
            devices_by_serial = {device['serial']: device for device in device_list}
            for serial in args.serials:
                if serial in devices_by_serial:
                    remove_list.append({"serial": serial, "networkId": devices_by_serial[serial]["networkId"]})

    elif args.input_file:
        # If the network_id_list is empty, we need to get all devices in the input file
        for serial, data in device_by_serial.items():
            device_data = data["paths"]["/"]
            remove_list.append({"serial": serial, "networkId": device_data["networkId"]})

    if remove_list:
        print (remove_list)
        answer = prompt_user(f"Remove these devices from their networks?", ["Yes", "No"])
        if answer == "Yes":
            for item in remove_list:
                if args.reboot:
                    event_handler("info", f"Rebooting {item['serial']}.")
                    response = dashboard.devices.rebootDevice(item['serial'])
                    event_handler("debug", response)
                event_handler("info", f"Removing {item['serial']} from network {item['networkId']}")
                response = dashboard.networks.removeNetworkDevices(item['networkId'], item['serial'])
                event_handler("debug", response)
    else:
        event_handler("warning", "No devices to remove.")

def release_command(args):
    device_list = []
    release_list = []

    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, simulate=args.dry_run, output_log=False)

    # If a data file is provided, use that data instead of making an API call
    if args.input_file:
        import_data = import_file(args.input_file)
        for serial, device_data in import_data["devices"].items():
            if "/" in device_data["paths"]:
                device_list.append(serial)
    elif args.serials:
        device_list = args.serials
    else:
        event_handler("critical", "Input file or serials argument must be provided.")

    #
    # Check to make sure these these devices are in the inventory
    #
    inventory = dashboard.organizations.getOrganizationInventoryDevices(args.org_id, total_pages='all')
    inventory_serials = [item["serial"] for item in inventory]
    for serial in device_list:
        if serial in inventory_serials:
            release_list.append(serial)
        else:
            event_handler("error", f"Serial {serial} not found in inventory.")

    if release_list:
        print (release_list)
        answer = prompt_user(f"Release these devices from inventory?", ["Yes", "No"])
        if answer == "Yes":
            response = dashboard.organizations.releaseFromOrganizationInventory(
                args.org_id, 
                serials=release_list
            )
            event_handler("debug", response)
    else:
        event_handler("warning", "No devices to release.")

def create_command(args):
    # For the create commands, we are going to use the meraki python library to get the data
    # because it makes it a bit easeier and we do not have to worry about the API paths
    # not being there.
    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url, simulate=args.dry_run,)

    if args.create_command == 'networks':
        try:
            response = dashboard.organizations.createOrganizationNetwork(
                args.org_id, args.name, args.product_types,
                tags=args.tags
                # timeZone='America/Los_Angeles', 
                # copyFromNetworkId='N_24329156', 
                # notes='Additional description of the network'
            )
        except meraki.exceptions.APIError as err:
            event_handler("error", err.message['errors'])
    elif args.create_command == 'client':
        import_data = import_file(args.input_file)
        clients_by_network = {}
        network_list = dashboard.organizations.getOrganizationNetworks(args.org_id, total_pages='all')
        networks_by_name = {network['name']: network for network in network_list}
        for client in import_data:
            if client["network"] in networks_by_name:
                network_id = networks_by_name[client["network"]]["id"]
                if networks_by_name[client["network"]]["isBoundToConfigTemplate"]:
                    template_id = networks_by_name[client["network"]]["configTemplateId"]
                else:
                    template_id = None
                if network_id not in clients_by_network:
                    clients_by_network[network_id] = {
                        'template_id': template_id,
                        'network': networks_by_name[client["network"]]["name"],
                        'clients': []
                    }
                clients_by_network[network_id]["clients"].append(client)
            else:
                event_handler("error", f"Network {client['network']} not found in organization {args.org_id}")

        for network_id, data in clients_by_network.items():
            if data["template_id"]:
                use_network_id = data["template_id"]
            else:
                use_network_id = network_id
            network_group_policies = dashboard.networks.getNetworkGroupPolicies(use_network_id)
            group_policy_by_name = {network_group_policy['name']: network_group_policy for network_group_policy in network_group_policies}
            macs_by_policy = {}
            for client in data["clients"]:
                clients = [{'mac': client["mac"], 'name': client["name"]}]
                device_policy = 'Group policy'
                group_policy_name = client["assigned"][0]["name"]
                if group_policy_name not in macs_by_policy:
                    macs_by_policy[group_policy_name] = []
                macs_by_policy[group_policy_name].append(client["mac"])
            print(data["network"])
            for group_policy_name, macs in macs_by_policy.items():
                print(group_policy_name)
                print(" ".join(macs))
                # print (f"{client['name']}, {client['mac']} {group_policy_name}")
                # if group_policy_name in group_policy_by_name:
                #     groupPolicyId = group_policy_by_name[group_policy_name]["groupPolicyId"]
                #     response = dashboard.networks.provisionNetworkClients(
                #         network_id, clients, device_policy, groupPolicyId=groupPolicyId
                #         # policiesBySecurityAppliance={'devicePolicy': 'Normal'}, 
                #         # policiesBySsid={'0': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '1': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '2': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '3': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '4': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '5': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '6': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '7': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '8': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '9': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '10': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '11': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '12': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '13': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}, '14': {'devicePolicy': 'Group policy', 'groupPolicyId': '101'}}
                #     )
                # else:
                #     event_handler("error", f"Group policy {group_policy_name} not found in network {network_id}")
                #     continue


def update_command(args):
    # For the create commands, we are going to use the meraki python library to get the data
    # because it makes it a bit easeier and we do not have to worry about the API paths
    # not being there.
    dashboard = meraki.DashboardAPI(api_key=args.api_key, base_url=args.base_url)

    if args.update_command == 'client-policy':
        import_data = import_file(args.input_file)
        network_list = dashboard.organizations.getOrganizationNetworks(args.org_id, total_pages='all')
        networks_by_name = {network['name']: network for network in network_list}
        for client in import_data:
            if client["network"] in networks_by_name:
                network_id = networks_by_name[client["network"]]["id"]
                print (f"Found {client['clientId']} in network {network_id}")
                response = dashboard.networks.updateNetworkClientPolicy(
                    network_id, client_id, device_policy, 
                    groupPolicyId='101'
                )
            else:
                event_handler("error", f"Network {client['network']} not found in organization {args.org_id}")
            print(client)


def prompt_user(message, options):
    """
    Prompt the user with a message and a list of options.
    
    Args:
        message (str): The prompt message.
        options (list): A list of valid options for the user to choose from.
    
    Returns:
        str: The option chosen by the user.
    """
    options_str = "/".join(options)  # Format the options for display (e.g., Yes/No)
    
    while True:
        user_input = input(f"{message} ({options_str}): ").strip()
        
        if user_input in options:
            return user_input
        else:
            print(f"Invalid input. Please choose one of the following: {options_str}")

def download_spec_command(args):
    parameters = ['version=3']
    spec = meraki_read_path("/organizations/{organizationId}/openapiSpec", args.api_key, args.base_url, parameters=parameters, organizationId=args.org_id)
    export_file(args.output_file, spec)

def event_handler(severity: str, message: str):
    """
    Handle events by logging messages and determining exit behavior based on severity.

    Args:
        message (str): The message to log.
        severity (str): The severity level of the message ('debug', 'info', 'warning', 'error', 'critical').
    """

    severity = severity.lower()
    
    if severity == 'debug':
        logger.debug(message)
    elif severity == 'info':
        logger.info(message)
    elif severity == 'warning':
        logger.warning(message)
        error_log.append(f"{severity}: {message}")
    elif severity == 'error':
        logger.error(message)
        error_log.append(f"{severity}: {message}")
        if args.exit_on_error:
            if args.trace_on_error:
                traceback.print_stack()
            sys.exit(1)
    elif severity == 'critical':
        error_log.append(f"{severity}: {message}")
        logger.critical(message)
        if args.trace_on_error:
            traceback.print_stack()
        sys.exit(1)
    else:
        logger.error(f"Unknown severity level: {severity}")
        if args.exit_on_error:
            sys.exit(1)

COLORS = {
    "green": "\033[0;32m",
    "yellow": "\033[0;33m",
    "orange": "\033[38;5;208m",
    "red": "\033[0;31m",
    "blue": "\033[0;34m",
    "reset": "\033[0m"
}

# Define log level colors
LOG_COLORS = {
    logging.DEBUG: "\033[0;37m",  # White
    logging.INFO: "\033[0;36m",   # Cyan
    logging.WARNING: COLORS["orange"],# Yellow
    logging.ERROR: "\033[0;31m",  # Red
    logging.CRITICAL: "\033[1;31m" # Bold Red
}

def color_message(message, color):
    return (f"{COLORS.get(color, COLORS['reset'])}{message}{COLORS['reset']}")

class color_formatter(logging.Formatter):
    def format(self, record):
        log_color = LOG_COLORS.get(record.levelno)
        record.msg = f"{log_color}{record.msg}\033[0m"
        return super().format(record)

def setup_logger(level):
    logger = logging.getLogger()
    handler = logging.StreamHandler(sys.stdout)
    formatter = color_formatter('%(levelname)s: %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(level)

def parse_app_args(arguments=None):
    # Create a top-level parser
    parser = argparse.ArgumentParser(
        description="""
msak: Meraki Data Tool

A command-line utility for exporting, importing, showing, creating, updating, claiming, removing, and releasing Meraki Dashboard configuration and inventory data.

Examples:
  msak export --org-id 12345 -o myorg.json
  msak import networks -i myorg.json --dry-run
  msak show devices --network "My Network"
  msak claim --network-id N_12345 --serials Q2XX-XXXX-XXXX Q2YY-YYYY-YYYY
  msak remove --networks "My Network" --serials Q2XX-XXXX-XXXX

For more details, run:
  msak --help
  msak <command> --help
  msak <command> <subcommand> --help
        """,
        epilog="""
Supported Meraki API Features:
- Organization, network, and device export/import (full or partial)
- Configuration templates
- Device inventory and status
- Network and device claims/unclaims
- Admin and client policy management
- Bulk operations with concurrency
- OpenAPI spec download and validation
- CSV/JSON/tabular output for reporting
        """,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('-b', '--base-url', type=str, default=os.getenv('MERAKI_BASE_URL', DEFAULT_MERAKI_BASE_URL), help='The base URL for the Meraki API (default: https://api.meraki.com/api/v1 or from $MERAKI_BASE_URL).')
    parser.add_argument('-k', '--api-key', type=str, default=os.getenv('MERAKI_DASHBOARD_API_KEY'), help='Meraki Dashboard API key (default: from $MERAKI_DASHBOARD_API_KEY).')
    parser.add_argument('-O', '--org-id', type=str, default=os.getenv('MERAKI_ORG_ID'), help='Meraki Organization ID (default: from $MERAKI_ORG_ID).')
    parser.add_argument('-c', '--max-concurrent-requests', type=int, default=5, help='Maximum concurrent API requests (default: 5).')
    parser.add_argument('--log-level', type=str, default='INFO', choices = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
                        help='Set logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL (default: INFO).')
    parser.add_argument('--exit-on-error', action='store_true', help="Exit immediately on error.")
    parser.add_argument('--trace-on-error', action='store_true', help="Print stack trace on error.")
    parser.add_argument('--spec-file', default=OPENAPI_SPEC_FILE, type=str, help='Path to OpenAPI spec file (default: meraki-openapi-spec.json).')

    # Immediately create a subparser holder for a sub-command.
    subparsers = parser.add_subparsers(dest='command', help='Main commands')

    # Subparser for `download-spec` command
    parser_download_spec = subparsers.add_parser("download-spec", help='Download the Meraki OpenAPI specification.')
    parser_download_spec.add_argument('-o', '--output_file', type=str, default=OPENAPI_SPEC_FILE, help='Output file for the spec (default: meraki-openapi-spec.json).')
    parser_download_spec.set_defaults(func=download_spec_command)

    # Subparser for `export` command
    parser_export = subparsers.add_parser("export", help='Export Meraki organization, network, and device configuration to a JSON file.')
    parser_export.set_defaults(func=export_command)
    parser_export.add_argument('-o', '--output_file', type=str, help='Output file for export.')
    parser_export.add_argument('--org-id', help='Organization ID to export.', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_export.add_argument('--tags', nargs='+', help='Filter networks by tags.')
    parser_export.add_argument('--networks', nargs='+', help='Filter by network names.')
    parser_export.add_argument('--network_ids', nargs='+', help='Filter by network IDs.')
    parser_export.add_argument('--full', help='Export all available paths.', action='store_true')
    parser_export.add_argument('--skip-templates', help='Skip exporting templates.', action='store_true')
    parser_export.add_argument('--contexts', nargs='+', default=['organizations','networks','devices'], choices=['organizations','networks','devices', 'inventory'], help='API base paths to export (default: organizations,networks,devices,inventory).')

    # Subparser for `import`
    parser_import = subparsers.add_parser("import", help='Import configuration from a previously exported JSON file.')
    parser_import.set_defaults(func=import_command)
    import_subparsers = parser_import.add_subparsers(dest='import_command', help='Resources to import')

    # Subcommand: `import organizations`
    parser_import_organizations = import_subparsers.add_parser('organizations', help='Import organization-level configuration.')
    parser_import_organizations.add_argument('--source-org-id', help='Source org ID in the import file.')
    parser_import_organizations.add_argument('-i', '--input-file', type=str, required=True, help='Input JSON file (required).')
    parser_import_organizations.add_argument('--dry-run', help='Simulate import, do not make changes.', action='store_true')
    parser_import_organizations.add_argument('--diff', help='Show diffs before applying changes.', action='store_true')
    parser_import_organizations.add_argument('--subpath', type=str, help='Only import a specific subpath.')
    # Subcommand: `import networks`
    parser_import_networks = import_subparsers.add_parser('networks', help='Import network-level configuration.')
    parser_import_networks.add_argument('--dry-run', help='Simulate import.', action='store_true')
    parser_import_networks.add_argument('-i', '--input-file', type=str, required=True, help='Input JSON file (required).')
    parser_import_networks.add_argument('--source-network-ids', nargs='+', help='Only import these network IDs.')    
    parser_import_networks.add_argument('--diff', help='Show diffs.', action='store_true')
    parser_import_networks.add_argument('--always-write', help='Ignore diffs and always write.', action='store_true')
    parser_import_networks.add_argument('--create-only', help='Only create/bind networks, do not update.', action='store_true')   
    parser_import_networks.add_argument('--subpath', type=str, help='Only import a specific subpath.')
    # Subcommand: `import devices`
    parser_import_devices = import_subparsers.add_parser('devices', help='Import device-level configuration.')
    parser_import_devices.add_argument('-i', '--input-file', type=str, required=True, help='Input JSON file (required).')
    parser_import_devices.add_argument('--ignore-profile', help='Ignore switch port profiles.', action='store_true')
    parser_import_devices.add_argument('-p', '--product-types', nargs='+', choices=PRODUCT_TYPES, help='Only import these product types.')
    parser_import_devices.add_argument('--diff', help='Show diffs.', action='store_true')
    parser_import_devices.add_argument('--always-write', help='Ignore diffs and always write.', action='store_true')
    parser_import_devices.add_argument('--dry-run', help='Simulate import.', action='store_true')
    parser_import_devices.add_argument('--source-network-ids', nargs='+', help='Only import devices from these networks.')
    parser_import_devices.add_argument('--serials', nargs='+', help='Only import these serials.')
    # Subcommand: `import templates`
    parser_import_templates = import_subparsers.add_parser('templates', help='Import configuration templates.')
    parser_import_templates.add_argument('--dry-run', help='Simulate import.', action='store_true')
    parser_import_templates.add_argument('-i', '--input-file', type=str, required=True, help='Input JSON file (required).')
    parser_import_templates.add_argument('--diff', help='Show diffs.', action='store_true')
    parser_import_templates.add_argument('--always-write', help='Ignore diffs and always write.', action='store_true')
    parser_import_templates.add_argument('--subpath', type=str, help='Only import a specific subpath.')

    # Subparser for `show` command
    parser_show = subparsers.add_parser('show', help='Display information from the Meraki Dashboard or from an export file.')
    parser_show.add_argument('--json', help='Output as JSON.', action='store_true')
    parser_show.set_defaults(func=show_command)
    show_subparsers = parser_show.add_subparsers(dest='show_command', help='Resources to show')

    # Subcommand: `show networks`
    parser_show_networks = show_subparsers.add_parser('networks', help='Show networks in the organization.')
    parser_show_networks.add_argument('--org-id', help='Organization ID.', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_show_networks.add_argument('--tags', nargs='+', help='Filter by tags.')
    parser_show_networks.add_argument('--json', help='Output as JSON.', action='store_true')
    parser_show_networks.add_argument('-i', '--input-file', type=str, help='Use data from export file.')

    # Subcommand: `show organizations`
    parser_show_organizations = show_subparsers.add_parser('organizations', help='Show organizations accessible by the API key.')

    # Subcommand: `show devices`
    parser_show_devices = show_subparsers.add_parser('devices', help='Show devices in the organization.')
    parser_show_devices.add_argument('--network', type=str, help='Filter by network name.')
    parser_show_devices.add_argument('--serial', type=str, help='Filter by Serial.')
    parser_show_devices.add_argument('--org-id', help='Organization ID.', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_show_devices.add_argument('-i', '--input-file', type=str, help='Use data from export file.')
    parser_show_devices.add_argument('--json', help='Output as JSON.', action='store_true')
    parser_show_devices.add_argument('--csv', help='Output as CSV.', action='store_true')
    parser_show_devices.add_argument('-o', '--output-file', type=str, default='devices.csv', help='Output file for devices.')

    # Subcommand: `show inventory`
    parser_show_inventory = show_subparsers.add_parser('inventory', help='Show inventory devices.')
    parser_show_inventory.add_argument('--org-id', help='Organization ID.', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_show_inventory.add_argument('--serials', nargs='+', help='Filter by Serials.')
    parser_show_inventory.add_argument('-i', '--input-file', type=str, help='Use data from export file.')
    parser_show_inventory.add_argument('--json', help='Output as JSON.', action='store_true')
    parser_show_inventory.add_argument('--csv', help='Output as CSV.', action='store_true')
    parser_show_inventory.add_argument('-o', '--output-file', type=str, default='inventory.csv', help='Output file for inventory.')

    # Subcommand: `show templates`
    parser_show_templates = show_subparsers.add_parser('templates', help='Show configuration templates.')
    parser_show_templates.add_argument('--org-id', help='Organization ID.', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_show_templates.add_argument('-i', '--input-file', type=str, help='Use data from export file.')
    parser_show_templates.add_argument('--json', help='Output as JSON.', action='store_true')

    # Subcommand: `show client-policy`
    parser_show_clients = show_subparsers.add_parser('client-policy', help='Show client policy assignments.')
    parser_show_clients.add_argument('--org-id', help='Organization ID.', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_show_clients.add_argument('--json', help='Output as JSON.', action='store_true')
    parser_show_clients.add_argument('--timespan', type=int, default=7, help='Timespan in days (default: 7).')
    parser_show_clients.add_argument('-o', '--output-file', type=str, help='Output file for client policy.')

    # Subcommand: `show me`
    parser_show_me = show_subparsers.add_parser('me', help='Show current API user and org context.')
    parser_show_me.add_argument('--org-id', help='Organization ID.', default=os.getenv('MERAKI_ORG_ID'), type=str)    

    # Subcommand: `show admins`
    parser_show_admins = show_subparsers.add_parser('admins', help='Show organization admins.')
    parser_show_admins.add_argument('-i', '--input-file', type=str, help='Use data from export file.')
    parser_show_admins.add_argument('--json', help='Output as JSON.', action='store_true')
    parser_show_admins.add_argument('--csv', help='Output as CSV.', action='store_true')
    parser_show_admins.add_argument('-o', '--output_file', type=str, help='Output file for admins.')

    parser_import.add_argument('--source-network-id', help='Source network ID.', type=str)
    parser_import.add_argument('--source-org-id', help='Source org ID.', type=str)

    # Subparser for `create` command
    parser_create = subparsers.add_parser('create', help='Create new resources in the Meraki Dashboard.')
    parser_create.set_defaults(func=create_command)
    create_subparsers = parser_create.add_subparsers(dest='create_command', help='Create resources')

    # Subcommand: `create networks`
    parser_create_networks = create_subparsers.add_parser('networks', help='Create a new network.')
    parser_create_networks.add_argument('--org-id', help='Organization ID.', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_create_networks.add_argument('--name', help='Network name (required).', required=True, type=str)
    parser_create_networks.add_argument('--product-types', nargs='+', default=['appliance', 'switch', 'wireless'], help="Product types (default: appliance, switch, wireless).")
    parser_create_networks.add_argument('--tags', nargs='+', default=[], help="Tags.")
    parser_create_networks.add_argument('--dry-run', help='Simulate creation.', action='store_true')

    # Subcommand: `create client`
    parser_create_clients = create_subparsers.add_parser('client', help='Create client policy assignments from a file.')
    parser_create_clients.add_argument('--org-id', help='Organization ID.', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_create_clients.add_argument('-i', '--input-file', required=True, type=str, help='Input JSON file (required).')
    parser_create_clients.add_argument('--dry-run', help='Simulate creation.', action='store_true')

    # Subparser for `update` command
    parser_update = subparsers.add_parser('update', help='Update resources in the Meraki Dashboard.')
    parser_update.set_defaults(func=update_command)
    update_subparsers = parser_update.add_subparsers(dest='update_command', help='Update resources')

    # Subcommand: `update client-policy`
    parser_update_clients = update_subparsers.add_parser('client-policy', help='Update client policy assignments from a file.')
    parser_update_clients.add_argument('--org-id', help='Organization ID.', default=os.getenv('MERAKI_ORG_ID'), type=str)
    parser_update_clients.add_argument('-i', '--input-file', required=True, type=str, help='Input JSON file (required).')

    # Subparser for `release` command
    parser_release = subparsers.add_parser('release', help='Release devices from organization inventory.')
    parser_release.set_defaults(func=release_command)
    parser_release.add_argument('-i', '--input-file', type=str, help='Input JSON file.')
    parser_release.add_argument('--dry-run', help='Simulate release.', action='store_true')
    parser_release.add_argument('--serials', nargs='+', help='Serial numbers to release.')

    # Subparser for `remove` command
    parser_remove = subparsers.add_parser('remove', help='Remove devices from networks.')
    parser_remove.set_defaults(func=remove_command)
    parser_remove.add_argument('-i', '--input-file', type=str, help='Input JSON file.')
    parser_remove.add_argument('--dry-run', help='Simulate removal.', action='store_true')
    parser_remove.add_argument('--reboot', help='Reboot before removal.', action='store_true')
    parser_remove_group = parser_remove.add_mutually_exclusive_group()
    parser_remove_group.add_argument('--network-ids', help='Source network IDs.', type=str)
    parser_remove_group.add_argument('--networks', nargs='+', help='Source networks.')
    parser_remove_group.add_argument('--serials', nargs='+', help='Serial numbers to remove.')

    # Subparser for `unclaim` command
    parser_unclaim = subparsers.add_parser('unclaim', help='Unclaim devices from networks and inventory.')
    parser_unclaim.set_defaults(func=unclaim_command)
    parser_unclaim.add_argument('-i', '--input-file', type=str, help='Input JSON file.')
    parser_unclaim.add_argument('--dry-run', help='Simulate unclaim.', action='store_true')
    parser_unclaim_group = parser_unclaim.add_mutually_exclusive_group()
    parser_unclaim_group.add_argument('--network-ids', help='Source network IDs.', type=str)
    parser_unclaim_group.add_argument('--networks', nargs='+', help='Source networks.')
    parser_unclaim_group.add_argument('--serials', nargs='+', help='Serial numbers to unclaim.')

    # Subparser for `claim` command
    parser_claim = subparsers.add_parser("claim", help='Claim devices into inventory or a network.')
    parser_claim.set_defaults(func=claim_command)
    parser_claim.add_argument('-i', '--input-file', type=str, help='Input JSON file.')
    parser_claim.add_argument('--dry-run', help='Simulate claim.', action='store_true')
    parser_claim.add_argument('--network-id', help='Target network ID.', type=str)
    parser_claim.add_argument('--network', help='Target network name.', type=str)
    parser_claim.add_argument('--source-network-id', help='Source network ID.', type=str)
    parser_claim.add_argument('--source-network-name', help='Source network name.', type=str)    
    parser_claim.add_argument('--serials', nargs='+', help='Serial numbers to claim.')
    parser_claim.add_argument('--inventory', help='Claim into organization inventory.', action='store_true')
    parser_claim.add_argument('--batch-size', help='Batch size for claims (default: 50)', default=50, type=int)

    return parser.parse_args(arguments)    

async def main(args):
    # Execute the function for the subcommand
    if args.command =='export':
        await export_command(args)
    else:
        if hasattr(args, 'func'):
            args.func(args)
    # else:
    #     parser.print_help()

if __name__ == "__main__":
    suppress_logging = False
    output_log = False
    log_path = os.path.join(os.getcwd(), "log")
    if not os.path.exists(log_path):
        os.makedirs(log_path)

    error_log = []
    args = parse_app_args()
    log_level = getattr(logging, args.log_level.upper(), logging.INFO)
    setup_logger(log_level)
    logger = logging.getLogger()

    config_file = os.path.dirname(os.path.realpath(__file__)) + '/msak.yaml'
    if os.path.exists(config_file):
        config = import_file(config_file)
    
    openapi_spec = import_file(args.spec_file)
    logger.debug(args)

    asyncio.run(main(args))
